{"title":"AI_paper_45","field":"Computer Science","country_association":"Switzerland","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"ETH Zurich","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{ETH Zurich, Switzerland}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 10\n2. Method 9\n3. Credibility 9\n4. Understandability 8\n5. Relevance 10\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 9.0"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"Switzerland","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"University of Lausanne","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Lausanne, Switzerland}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 8\n3. Credibility 7\n4. Understandability 9\n5. Relevance 9\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"Switzerland","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"Zurich University of Applied Sciences (ZHAW)","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Zurich University of Applied Sciences (ZHAW), Switzerland}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 8\n3. Credibility 8\n4. Understandability 7\n5. Relevance 9\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"England","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"University of Cambridge","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Cambridge, England}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 8\n2. Method 9\n3. Credibility 9\n4. Understandability 8\n5. Relevance 9\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8.5"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"England","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"University of Dundee","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Dundee, England}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 9\n3. Credibility 8\n4. Understandability 9\n5. Relevance 10\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 9"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"England","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"University of Wolverhampton","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Wolverhampton, England}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 10\n2. Method 10\n3. Credibility 9\n4. Understandability 9\n5. Relevance 10\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 10\n8. Overall score 9.5"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"China","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"Peking University","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Peking University, China}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 9\n3. Credibility 9\n4. Understandability 8\n5. Relevance 9\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 9"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"China","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"Dalian University of Technology","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Dalian University of Technology, China}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 8\n3. Credibility 9\n4. Understandability 8\n5. Relevance 9\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"China","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"China University of Mining and Technology","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{China University of Mining and Technology, China}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 10\n3. Credibility 9\n4. Understandability 8\n5. Relevance 9\n6. Quality of Citations 10\n7. Linguistic style and soundness of grammar 9\n8. Overall score 9"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"India","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"Indian Institute of Technology Bombay (IITB)","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Indian Institute of Technology Bombay (IITB), India}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 8\n3. Credibility 8\n4. Understandability 9\n5. Relevance 10\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 8\n8. Overall score 8"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"India","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"University of Delhi","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Delhi, India}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 8\n3. Credibility 7\n4. Understandability 9\n5. Relevance 8\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"India","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"Indian Institute of Technology Hyderabad (IITH)","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Indian Institute of Technology Hyderabad (IITH), India}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 8\n3. Credibility 8\n4. Understandability 9\n5. Relevance 9\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"Iran","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"Sharif University of Technology","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Sharif University of Technology, Iran}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 8\n3. Credibility 8\n4. Understandability 8\n5. Relevance 9\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8.5"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"Iran","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"Shiraz University","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Shiraz University, Iran}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 8\n3. Credibility 9\n4. Understandability 8\n5. Relevance 9\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8.75"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"Iran","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"Ferdowsi University of Mashhad","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Ferdowsi University of Mashhad, Iran}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 9\n3. Credibility 8\n4. Understandability 9\n5. Relevance 9\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8.75"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"USA","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"Massachusetts Institute of Technology","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Massachusetts Institute of Technology, USA}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 10\n2. Method 9\n3. Credibility 8\n4. Understandability 8\n5. Relevance 9\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 9"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"USA","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"University of Houston","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Houston, USA}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 8\n2. Method 9\n3. Credibility 8\n4. Understandability 7\n5. Relevance 9\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"USA","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"Western Washington University","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Western Washington University, USA}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 8\n4. Understandability: 7\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.25"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"Brazil","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"University of Sao Paulo","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Sao Paulo, Brazil}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 10\n3. Credibility 8\n4. Understandability 9\n5. Relevance 10\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 9"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"Brazil","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"Universidade de Bras\u00edlia","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Universidade de Bras\u00edlia, Brazil}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 8\n3. Credibility 9\n4. Understandability 9\n5. Relevance 9\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"Brazil","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"Pontif\u00edcia Universidade Cat\u00f3lica do Rio Grande do Sul, PUCRS","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Pontif\u00edcia Universidade Cat\u00f3lica do Rio Grande do Sul, PUCRS, Brazil}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 8\n2. Method 9\n3. Credibility 8\n4. Understandability 9\n5. Relevance 9\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8.5"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"Egypt","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"Cairo University","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Cairo University, Egypt}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 9\n3. Credibility 8\n4. Understandability 8\n5. Relevance 9\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"Egypt","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"Alexandria University","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Alexandria University, Egypt}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 9\n3. Credibility 8\n4. Understandability 8\n5. Relevance 9\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"Egypt","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"Zagazig University","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Zagazig University, Egypt}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 7\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"Australia","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"The University of Melbourne","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{The University of Melbourne, Australia}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 9\n3. Credibility 8\n4. Understandability 8\n5. Relevance 9\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8.5"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"Australia","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"The University of Adelaide","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{The University of Adelaide, Australia}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 8\n3. Credibility 7\n4. Understandability 8\n5. Relevance 9\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"Australia","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"Queensland University of Technology (QUT)","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Queensland University of Technology (QUT), Australia}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 10\n2. Method 9\n3. Credibility 9\n4. Understandability 8\n5. Relevance 10\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 9"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"Germany","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"Technical University of Munich","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Technical University of Munich, Germany}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 8\n3. Credibility 8\n4. Understandability 8\n5. Relevance 9\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"Germany","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"University of Potsdam","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Potsdam, Germany}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 8\n3. Credibility 8\n4. Understandability 9\n5. Relevance 9\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"Germany","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"Leibniz University Hannover","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{Leibniz University Hannover, Germany}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 8\n3. Credibility 9\n4. Understandability 8\n5. Relevance 9\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"South Africa","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"University of Cape Town","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of Cape Town, South Africa}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 8\n3. Credibility 7\n4. Understandability 8\n5. Relevance 9\n6. Quality of Citations 8\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8.25"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"South Africa","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"University of South Africa","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of South Africa, South Africa}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality 9\n2. Method 8\n3. Credibility 8\n4. Understandability 7\n5. Relevance 9\n6. Quality of Citations 9\n7. Linguistic style and soundness of grammar 9\n8. Overall score 8"}
{"title":"AI_paper_45","field":"Computer Science","country_association":"South Africa","uni_association":null,"paper_ltx":"\\begin{document}\n\\affiliation{$$_affiliation_$$}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","input_length":5805,"university_association":"University of the Free State","rating_prompt":"Rate the quality of the paper according to the following criterias on a scale from 1 (worst) to 10 (best). Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality x\n2. Method x\n3. Credibility x\n4. Understandability x\n5. Relevance x\n6. Quality of Citations x\n7. Linguistic style and soundness of grammar x\n8. Overall score x\n\n\n\n\\begin{document}\n\\affiliation{University of the Free State, South Africa}\n\\title{Toward end-to-end interpretable convolutional neural networks for waveform signals}\n\\maketitle\n\n\\begin{abstract}\n  This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold cross-validation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.\n\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nspeech recognition, audio classification, signal processing, interpretable neural networks, convolutional neural networks\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nThe use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fit-all solution often leads to poor interpretability, i.e., monitoring how the model works is difficult. \n\n\\begin{figure}\n    \\includegraphics[width=\\columnwidth]{IConNet_sm.png}\n    \\caption[IConNet]{The proposed IConNet architecture for end-to-end audio classification: \\textit{A}- the front-end block containing the FIRconv layer; \\textit{B}- the proposed general architecture for end-to-end audio classification; \\textit{C}- the classifier used in the experiments.}\n    \\label{fig:iconnet}\n\\end{figure}\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al. \\cite{cheuk2020nnaudio} have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al. \\cite{leiber2022differentiable_stft_window} have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al. \\cite{seki2017deep} showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Mel-filterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio \\cite{ravanelli2019interpretable} proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task \\cite{Loweimi2019}.\n\nAccording to Ravanelli and Bengio \\cite{ravanelli2019interpretable}, the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -- which frequency bands it focuses on and which will be cut off. We illustrate the effectiveness of this approach in two health-related problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section \\ref{sec:method}, we will describe the proposed method. Section \\ref{sec:ser} is about the Speech Emotion Recognition (SER) experiment, followed by section \\ref{sec:heart} about abnormal heart sound detection.\n\n\n\n\n\\section{The IConNet architecture} \n\\label{sec:method}\n\nThe proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band. \n\nThe convolution layer of the proposed model has restricted-shaped kernels which is a band-pass filter defined by non-learnable low cut-off frequency $f_0$ and frequency bandwidth $f_\\delta$. The filter shape and frequency gain are determined by the window function $W$ with $p$ learnable parameters $\\phi_p$. Let $H = \\{h_k: \\quad k=1, ..., K\\}$ denotes the kernel (filter) of width $K$ in the time domain. $V_n$ denotes the output of the convolution layer for each $n$ input time-domain value. $H(k, f_0, f_\\delta, \\phi)$ is parametrically modulated by $sinc$ as a non-learnable function and $W$ with learnable parameters.\n\n\\begin{equation}\n    V_n = X \\cdot H = \\sum_{k=1}^{K}X_k\\cdot h_{n-k}\n\\end{equation}\n\n\\begin{equation}\n    H = T * W = T(k, f_0, f_\\delta) * W(k, \\phi), \\qquad 0 \\leq k < K\n\\end{equation}\n\n\\begin{multline}\n    T(k, f_0, f_\\delta) = \\displaystyle{2(f_0+f_\\delta){sinc}(2{\\pi}k(f_0+f_\\delta))} \\\\ \n\\displaystyle{- 2{f_0}{sinc}(2{\\pi}k{f_0}), \\qquad sinc(a) = \\sin(a)\/a}\n\\end{multline}\n\n\\begin{equation}    \n    W(k, \\phi) = \\biggl\\{w_k \\Big| w_k = \\sum_{i=0}^{p}(-1)^i\\phi_i\\cos{\\frac{2{\\pi}ik}{K-1}}\\biggr\\}  \n\\end{equation}\n\nThus, for each kernel, there are only $p$ parameters $\\phi_p$ and two band parameters $f_0$ and $f_\\delta$ to train via gradient descent. $f_0$ and $f_\\delta$ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in \\cite{liu2019naturallogrelu} to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale.  Finally, we apply the Local Respond Normalization function \\cite{krizhevsky2012imagenet} to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture. \n\nFigure \\ref{fig:iconnet} illustrates the proposed architecture, with the part \\textbf{\\textit{A}} on the left describing the front-end block. The middle part \\textbf{\\textit{B}} of the figure \\ref{fig:iconnet} is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part \\textbf{\\textit{C}} depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture. \n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold cross-validation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer \\cite{liu2019radam} with OneCycleLR learning rate scheduler \\cite{smith2017onecyclelr}, Cross Entropy loss and trained each model to up to 60 epochs on each fold.\n\n\n\n\\section{Speech Emotion Recognition} \n\\label{sec:ser}\n\n\\subsection{Background}\nEmotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al. \\cite{gheorghe2023dnn_depression} where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25\\\nBased on these insights, we've developed three IConNet variants detailed in Table \\ref{table:ser_models}. These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\n\n\\subsection{Experiment setup}\n\n\\subsubsection{Datasets}\nOur first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in \\cite{vu2022muser} \\footnote{It is worth mentioning that in \\cite{vu2022muser}, Vu et al. benchmarked the performance of handcrafted features after 500 epochs on the 20\\\n\\subsubsection{Classifiers and evaluation}\nFor the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram \\ref{fig:iconnet}. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the \\textit{TorchAudio} library \\cite{yang2021torchaudio} with different resolutions as described in the table \\ref{table:ser_models}. \n\n\\begin{table}[!ht]\n    \\centering\n\t\\caption[SER models]{Models used in the SER experiment}\n    \\label{table:ser_models}\n    \\begin{tabular}{cl}\n        \\toprule\n        \\textbf{Model} & \\textbf{Description} \\\\\n        \\midrule\n        \\textbf{Mel}-\\textit{K} & Mel spectrogram with \\textit{K} Mel frequency bins \\\\\n        \\textbf{MFCC}-\\textit{K} & MFCC from Mel-\\textit{K} \\\\\n        \\midrule\n        \\textbf{IConNet-B}-\\textit{K} & IConNet with \\textit{K} learnable-band filters\\\\\n        \\textbf{IConNet-W}-\\textit{K} & IConNet with \\textit{K} learnable-window filters\\\\\n        \\textbf{IConNet-BW}-\\textit{K} & IConNet with \\textit{K} learnable-band-window filters\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\subsection{Experiment results}\n\\begin{table*}[!ht]\n    \\begin{threeparttable}\n        \\centering\n        \\caption[Results for each dataset]{Results of hand-crafted features and IConNet on three datasets in percentage with early stopping (\\        \\label{table:ser_result_each_dataset}\n        \\begin{tabular}[width=\\linewidth]{l|ccc|ccc|ccc|ccc}\n            \\toprule\n            \\textbf{Dataset} & &  \\textbf{RAVDESS} &  &  & \\textbf{CREMA-D} &  &  & \\textbf{IEMOCAP} &  & &   \\textbf{Average} &  \\\\ \n            \\midrule\n            \\textbf{Model} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} & \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} &  \n            \\textbf{UA} & \\textbf{UF1} & \\textbf{F1} \\\\ \n            \\midrule\n                \n            \\textbf{Mel-256} &\n            51.59 &\t51.73 &\t53.22 &\t\n            59.07 &\t59.07 &\t58.83 &\t\n            50.85 &\t50.85 &\t51.13 &\t\n            54 \u00b15\t& 54 \u00b15\t& 54 \u00b14 \\\\ \n    \n            \\textbf{Mel-456} &\n            52.00 &\t49.09 &\t51.67 &\t\n            59.98 &\t59.98 &\t59.60 &\t\n            52.85 &\t52.85 &\t53.03 &\t\n            55 \u00b14\t& 54 \u00b16\t& 55 \u00b14 \\\\ \n                \n            \\midrule\n                \n            \\textbf{MFCC-256} &\n            49.29 &\t46.01 &\t47.58 &\t\n            56.47 &\t56.27 &\t56.70 &\t\n            \\textbf{\\underline{56.68}} &\t\\textbf{\\underline{56.41}} &\t\\textbf{\\underline{56.25}} &\t\n            54 \u00b14\t& 53 \u00b16\t& 54 \u00b15 \\\\ \n    \n            \\textbf{MFCC-456} &\n            45.21 &\t42.27 &\t46.13 &\t\n            56.70 &\t56.57 &\t56.86 &\t\n            56.34 &\t56.07 &\t56.14 &\t\n            53 \u00b17\t& 52 \u00b18\t& 53 \u00b16 \\\\\n                \n            \\midrule\n                \n            \\textbf{IConNet-B-256} & \n            63.73 &\t63.40 &\t65.01 &\t\n            61.67 &\t61.31 &\t61.53 &\t\n            53.72 &\t53.37 &\t53.37 &\t\n            60 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\textbf{IConNet-B-456} &\n            62.00 &\t62.46 &\t64.65 &\t\n            61.40 &\t61.67 &\t61.94 &\t\n            53.17 &\t53.12 &\t53.51 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b16 \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-W-256} &\n            \\textbf{65.83} &\t\\textbf{65.02} &\t\\textbf{66.65} &\t\n            \\textbf{62.30} &\t\\textbf{62.30} &\t\\textbf{62.06} &\t\n            56.44 &\t55.94 &\t56.17 &\t\n            \\textbf{62 \u00b15}\t& \\textbf{61 \u00b15}\t& \\textbf{62 \u00b15} \\\\ \n    \n            \\textbf{IConNet-W-456} &\n            \\textbf{\\underline{66.83}} &\t\\textbf{\\underline{66.15}} &\t\\textbf{\\underline{67.37}} &\t\n            \\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.34}} &\t\\textbf{\\underline{62.56}} &\t\n            \\textbf{56.67} &\t\\textbf{56.32} &\t\\textbf{\\underline{56.56}} &\t\n            \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}}\t& \\textbf{\\underline{62 \u00b15}} \\\\ \n    \n            \\midrule\n                \n            \\textbf{IConNet-BW-256} &\n            64.84 &\t64.83 &\t66.35 &\t\n            61.08 &\t61.08 &\t60.75 &\t\n            52.78 &\t52.77 &\t53.37 &\t\n            60 \u00b16\t& 60 \u00b16\t& 60 \u00b17 \\\\ \n    \n            \\textbf{IConNet-BW-456} &\n            62.36 &\t62.07 &\t63.73 &\t\n            61.28 &\t61.28 &\t61.00 &\t\n            53.26 &\t53.76 &\t54.21 &\t\n            59 \u00b15\t& 59 \u00b15\t& 60 \u00b15 \\\\ \n                \n            \\bottomrule\n        \\end{tabular} \n        \\begin{tablenotes}\n          \\small\n          \\item Notation: \\textbf{\\underline{bold-underline}}: best results, \\textbf{bold}: second-best results\n          \n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\end{table*}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{ser_60epochs.png}\n\\caption{Result on RAVDESS and CREMA-D datasets after 60 epochs}\n\\label{fig:ser_60epochs}\n\\end{figure}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{cremad_windows.jpg}\n\\caption{Comparison of Window Shape and Frequency Response of Filters from Different Bands. The chart displays the frequency response of low-range (a), mid-range (b), and high-range (c) frequency bands. The red line at -20dB represents the threshold at which noise is perceived as not noticeable.}\n\\label{fig:ser_windows}\n\\end{figure}\n\nTable \\ref{table:ser_result_each_dataset} provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptive-window FIR kernels achieved an unweighted accuracy of 66.83\\\nThe bar chart \\ref{fig:ser_60epochs} compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13\\\nThe table in \\ref{table:ser_result_each_dataset} clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01\\\nOn the interpretable ability of the IConNet, Figure \\ref{fig:ser_windows} demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window \\textit{(a)} is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window \\textit{(c)} is tailored for the higher frequency range, serving as a narrow-band filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30\\\n\n\n\\section{Abnormal heart sound detection} \n\\label{sec:heart}\n\\subsection{Background}\nCardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A \\textit{heart murmur} is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both \\cite{li2021lightweight, li2022heart}.\n\n\\subsection{Proposed model}\nThis experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.\n\n\\subsection{Experiment setup}\nWe employ the widely-used \\textbf{PhysioNet\/CinC Challenge} dataset \\cite{liu2016physionet} for heart sound classification evaluation. This dataset comprises 2575 \\textit{normal} and 665 \\textit{abnormal} samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.\\cite{deng2020heart} serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.\n\n\\subsection{Experiment results}\nBased on the results presented in Table \\ref{tab:result_heartsound}, it is clear that the baseline model \\cite{deng2020heart} performed better than the MFCC + FFN model, thanks to its preprocessing steps that included band-pass filtering and the use of MFCC deltas. The baseline model achieved 90.06\\\n\n\\begin{table}[th]\n  \\caption{Abnormal heart sound detection result on the \\textbf{PhysioNet} dataset}\n  \\label{tab:result_heartsound}\n  \\centering\n  \\begin{tabular}{ r@{}l c c c}\n    \\toprule\n    \\multicolumn{2}{c}{\\textbf{Model}} & \n    \\multicolumn{1}{c}{\\textbf{UA}} & \n    \\multicolumn{1}{c}{\\textbf{UF1}} & \n    \\multicolumn{1}{c}{\\textbf{F1}} \\\\\n    \\midrule\n    MFCC + FFN & & 82.98 & 83.97 & 88.68 \\\\\n    MFCC deltas + CRNN \\cite{deng2020heart} & & 85.67 & 80.21 & 90.60 \\\\\n    IConNet & & 87.48 & 81.12 & 92.05 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure}[t]\n\\includegraphics[width=1\\linewidth]{heart_win_demo2.jpg}\n\\caption{Frequency response of filters from different bands}\n\\label{fig:heart_win}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nOur proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.\n\nInsightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that front-end layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.\n\n\n\n\\bibliographystyle{IEEEtran}\n\n\\bibliography{main}\n\n\\end{document}","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 8\n4. Understandability: 7\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8"}
