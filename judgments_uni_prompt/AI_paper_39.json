{"title":"AI_paper_39","field":"Computer Science","country_association":"Switzerland","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"ETH Zurich","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at ETH Zurich in Switzerland. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.5"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"Switzerland","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"University of Lausanne","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Lausanne in Switzerland. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 9\n4. Understandability: 7\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"Switzerland","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"Zurich University of Applied Sciences (ZHAW)","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Zurich University of Applied Sciences (ZHAW) in Switzerland. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.5"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"England","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"University of Cambridge","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Cambridge in England. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 10\n4. Understandability: 9\n5. Relevance: 10\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 10\n8. Overall score: 9"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"England","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"University of Dundee","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Dundee in England. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8.5"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"England","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"University of Wolverhampton","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Wolverhampton in England. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 9\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.75"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"China","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"Peking University","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Peking University in China. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 8\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"China","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"Dalian University of Technology","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Dalian University of Technology in China. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 7\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 7\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8.0"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"China","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"China University of Mining and Technology","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at China University of Mining and Technology in China. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 9\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.75"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"India","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"Indian Institute of Technology Bombay (IITB)","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Indian Institute of Technology Bombay (IITB) in India. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 9\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.75"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"India","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"University of Delhi","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Delhi in India. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 10\n2. Method: 10\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 10\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 9.25"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"India","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"Indian Institute of Technology Hyderabad (IITH)","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Indian Institute of Technology Hyderabad (IITH) in India. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 8\n4. Understandability: 7\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"Iran","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"Sharif University of Technology","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Sharif University of Technology in Iran. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 10\n3. Credibility: 9\n4. Understandability: 9\n5. Relevance: 10\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 9.17"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"Iran","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"Shiraz University","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Shiraz University in Iran. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 7\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8.0"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"Iran","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"Ferdowsi University of Mashhad","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Ferdowsi University of Mashhad in Iran. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 9\n4. Understandability: 7\n5. Relevance: 10\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8.5"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"USA","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"Massachusetts Institute of Technology","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Massachusetts Institute of Technology in USA. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 10\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 9"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"USA","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"University of Houston","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Houston in USA. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.5"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"USA","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"Western Washington University","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Western Washington University in USA. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 10\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 9"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"Brazil","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"University of Sao Paulo","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Sao Paulo in Brazil. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.5"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"Brazil","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"Universidade de Bras\u00edlia","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Universidade de Bras\u00edlia in Brazil. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 7\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8.25"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"Brazil","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"Pontif\u00edcia Universidade Cat\u00f3lica do Rio Grande do Sul, PUCRS","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Pontif\u00edcia Universidade Cat\u00f3lica do Rio Grande do Sul, PUCRS in Brazil. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 7\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.25"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"Egypt","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"Cairo University","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Cairo University in Egypt. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 9\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.5"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"Egypt","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"Alexandria University","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Alexandria University in Egypt. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 8\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8.25"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"Egypt","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"Zagazig University","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Zagazig University in Egypt. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 8\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.5"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"Australia","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"The University of Melbourne","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at The University of Melbourne in Australia. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 8\n4. Understandability: 9\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.5"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"Australia","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"The University of Adelaide","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at The University of Adelaide in Australia. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 10\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 9"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"Australia","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"Queensland University of Technology (QUT)","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Queensland University of Technology (QUT) in Australia. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 8\n4. Understandability: 7\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"Germany","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"Technical University of Munich","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Technical University of Munich in Germany. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.5"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"Germany","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"University of Potsdam","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Potsdam in Germany. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 10\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 9\n5. Relevance: 10\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 9.17"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"Germany","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"Leibniz University Hannover","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Leibniz University Hannover in Germany. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.5"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"South Africa","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"University of Cape Town","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Cape Town in South Africa. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 9\n5. Relevance: 10\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 9."}
{"title":"AI_paper_39","field":"Computer Science","country_association":"South Africa","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"University of South Africa","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of South Africa in South Africa. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 9\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8.5"}
{"title":"AI_paper_39","field":"Computer Science","country_association":"South Africa","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","input_length":8292,"university_association":"University of the Free State","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of the Free State in South Africa. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{A Model-based Multi-Agent Personalized Short-Video Recommender System}\n\\begin{abstract}\n\nRecommender selects and presents top-K items to the user at each online request, and a recommendation session consists of several sequential requests. \nFormulating a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework has attracted increasing attention from both academic and industry communities. \nIn this paper, we propose a RL-based industrial short-video recommender ranking framework, which models and maximizes user watch-time in an environment of user multi-aspect preferences by a collaborative multi-agent formulization. Moreover, our proposed framework adopts a model-based learning approach to alleviate the sample selection bias which is a crucial but intractable problem in industrial recommender system.\nExtensive offline evaluations and live experiments confirm the effectiveness of our proposed method over alternatives. \nOur proposed approach has been deployed in our real large-scale short-video sharing platform, successfully serving over hundreds of millions users.\n\\end{abstract}\n\\maketitle\n\n\n\n\n\n\\section{Introduction}\nShort-video apps, such as TikTok, YoueTube Shorts, or Kuaishou, attract billions of users by creating a good user experience, which is to recommend short-form videos satisfying the user's interest.\nDuring each user visitation to a short-video app, the recommender generates request-based recommendation, and the sequential requests forming a continuous process named session.\nFor short-video recommendation, taking into account the long-term reward of each recommendation action and maximizing the cumulative satisfaction of each session have become increasingly important. \nFor the above aim, it has been improved to achieve more promising results to formulate a recommendation session as a Markov decision process and solving it by reinforcement learning (RL) framework.\nWhile the multi-aspect user preferences and sample selection bias have been seldom addressed publicly, which are two crucial issues in short-video recommendation.\n\nThe user experience is usually evaluated by the user's multi-aspect feedback on the recommended videos. A user provides multi-aspect responses implying her satisfaction at each recommended video, including WatchTime (the time spent on watching the video), and several explicit interactions: Follow (follow the author of the video), Like (like this video), Comment (provide comments on the video), etc.\nAt our industrial short-video sharing platform, the main goal is to optimize the cumulative WatchTime over a session, and the cumulative explicit interactions are the auxiliary goals, which is similar to \\cite{cai2023two}.\n\nThe existing work quite naturally takes the relation between WatchTime and explicit interactions as trade-off \\cite{cai2023two, chen2021reinforcement}, however, not all explicit interactions are in competition with WatchTime. For example, if a user comments a video, she tends to spend longer time on watching this video,  etc. More collaborative relations between WatchTime and other interactions could be found. To take this issue into consideration, we propose a multi-agent recommender ranking framework, and our framework introduces a collaborative environment where multiple intelligent agent components, each maximizing distinctive user preference, work together to handle the main goal maximizing the cumulative WatchTime over a session more effectively.\n\nBuilding a recommender agent offline from logged impressions with user feedback is the most practical and commonly used for the real industrial recommender, because performing online policy learning through interacting with real users usually harms user experiences. Using only logged impressions offline inevitably leads to Sample Selection Bias (SSB) \\cite{ESMM2018,XuZixuan2022SSB}, which is one common but challenging issue in industrial recommender. \nTake our industrial recommender as an illustration, the ranking model makes inference on the entire space with 400 candidates, and only 6 candidates are ranked out to present and get user feedback.\nNon-impression samples can also be logged, only without user feedback.\nTo alleviate SSB problem, we propose to extend our multi-agent framework into a model-based multi-agent framework. \nOur proposed framework alleviates SSB by introducing non-impression samples and simulating user feedback through a feedback fitting model. The feedback fitting model is optimized through iterative optimization.\n\nIn summary, the major contributions in this paper are:\n\\begin{itemize}[leftmargin=10pt]\n\\setlength{\\itemsep}{-1pt}\n    \\item We specify a collaborative multi-agent formulation to maximize user watch-time and explicit interactions in an environment of user multi-aspect preferences.\n    \\item We extend the multi-agent formulation into a model-based multi-agent framework to additionally address Sample Selection Bias. \n    \\item We verify the effectiveness of our proposed model-based multi-agent ranking framework through extensive experiments conducted on a public benchmark dataset and a large scale industrial dataset, and successfully apply it in a real-world large scale recommender system bringing a considerable performance improvement.\n\\end{itemize}\n\n\n\n\n\n\\section{The Proposed Method}\n\n\n\nWe detail the description of our method, the Model-based Multi-agent Ranking Framework (\\textbf{MMRF}), which aims at maximizing user WatchTime via a multi-agent collaborative planning, and alleviating SSB by introducing simulation on non-impression samples.\n\n\\subsection{Preliminary}\nSince multiple agents aim to collaboratively maximize WatchTime over a session, we formulate our problem as a multi-agent extension of Markov Decision Processes (MDPs) \\cite{MDP}. Formally, we take $N$ agents, in which the $N_{th}$ agent maximizes user WatchTime, and the rest $N-1$ agents are assigned auxiliary goals for the other user preferences. \\textbf{State} $S$ includes user profile, behavior history, request context and candidate item features shared among all agents. \\textbf{Action} $A = [A^1, ... , A^N]$ is the action sets, in which $a^i \\in A^i$ is the item score list generated by agent $i$ to determine the rank of all candidate items. \\textbf{Reward} $R = [R^1, ... , R^N]$, $R^i$ is agent $i$'s reward signal, which describes an individual dimension of user's multi-aspect preference.\n\nOur objective is to maximize the session-wise discounted total WatchTime \n$\\sum_{t=0}^T{\\gamma^{t} r_t^N}$, where $T$ is the time horizon, $\\gamma$ is discount factor and  $r_t^N$ indicates user's WatchTime in round $t$.\n\n\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{main-model.png}\n\\vspace{-2em}\n\\caption{An illustration of the implementation of our proposed MMRF.}\n\\label{main-model}\n\\vspace{-2em}\n\\end{figure}\n\n\\subsection{Multi-agent Collaboration Framework}\nSince user's time spent on video watching could be motivated by other explicit interactions, there exists a collaborative relationship across multiple aspects of user preference signals. \nBased on this intuition, we propose a collaboration framework across multiple agents as illustrated in Figure \\ref{main-model}. We selectively utilize knowledge from other auxiliary preference signals through attention mechanism, which is inspired by \\cite{iqbal2019actor}.\n\n\\textbf{Attentive Collaboration Mechanism} \nThe main idea behind our attention mechanism is to select beneficial information from other agents, so as to make a better action planning. \n\nFirstly, there are universal interactions among all agents by attention mechanism. For the $i_{th}$ agent, the beneficial information $e_t^i$, which is aggregated from the rest $N-1$ agents, could be formalized as:\n\\vspace{-0.3em}\n\\begin{small}\n\\begin{equation} \\label{eq1}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{e_t^i} = \\sum\\nolimits_{j \\neq i}{\\alpha_j \\ Relu(W_v h_t^j)}, \\ \\  \\alpha_j \\propto exp({h_t^j}^\\top W_k^\\top W_q h_t^i)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\\vspace{-0.3em}\nwhere $h_t^i$ is the state embedding of the $i_{th}$ agent. Weights $\\alpha_j$ calculate the similarity between agent $i$ and $j$'s representation through multi-head attention, in which $W_q$ transform $h_t^i$ as 'query' and $W_k$ transform $h_t^j$ as 'key'. \nNotably, the weights for extracting queries, keys and values are shared across all agents, which encourages a common embedding space. \n\n\nThen, with observed state $s_t$ as well as beneficial knowledge from other agents, the ranking action $a_t^k = \\pi_{\\theta_k}(s_t)$ of the $k_{th}$ auxiliary agent is formalized as: \n\\begin{small}\n\\begin{equation} \\label{eq10}\n\\begin{aligned}\n\\setlength{\\abovedisplayskip}{1pt}\n&\\bm{\\pi_{\\theta_k}}(s_t) = f_i(h_t^k, e_t^k)\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{aligned}\n\\end{equation}\n\\end{small}\nwhere $f_i(\\cdot)$ adopts a multi-layer perceptron, and generates ranking scores to meet auxiliary goals.\n\nFinally, after receiving the decision information from all the auxiliary agents, the agent for WatchTime makes its decision via action $a_t^N$ as:\n\\begin{small}\n\\begin{equation} \\label{eq2}\n\\setlength{\\abovedisplayskip}{1pt}\n\\bm{\\pi_{\\Theta}}(s_t) = f_N(h_t^N, e_t, a_t^{-N})\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $\\Theta = \\{\\theta_i\\}_{i=1..N}$ is a collection of $N$ actors' parameters since they need to work together to achieve the main goal. \nNot only intermediate decision knowledge $e_t = [e_t^1, ... , e_t^N]$ of multi-aspect preferences, but also direct decision results $a_t^{-N} = [a_t^1, ..., a_t^{N-1}]$ from $N-1$ auxiliary agents are integrated to support the WatchTime modeling. \n\n\\textbf{Policy Learning}\nWe adopt the deterministic policy gradient (DPG) \\cite{dpg} algorithm since our action space is continuous. \nIn detail, for agent $i$ with actor $\\pi_{\\theta_i}$, considering sample $(s_t, a_t^i, r_t^i, s_{t+1})$ from replay buffer $\\mathcal{D}_i$, the value function $Q_{\\phi_i}$ is estimated with temporal-difference error $\\delta_t^i = r_t^i + \\gamma Q_{\\phi_i}(s_{t+1}, \\pi_{\\theta_i}(s_{t+1}))- Q_{\\phi_i}(s_t,a_t^i)$, and the actor $\\pi_{\\theta_i}$ is updated by globally maximising $Q_{\\phi_i}$. \n\nSince we need to take advantages of auxiliary preference signals for the planning and maximising of WatchTime, the quality and accuracy of these auxiliary agents are also important. Thus, for auxiliary agents, the private auxiliary objective as well as the main objective for maximising $Q_{\\phi_N}$ are both considered. As a result, the gradient updating at round $t$ could be formalized as follows:\n\\begin{small}\n\\begin{equation} \\label{eq3}\n\\begin{aligned}\n& \\bm{\\phi_i^{(t+1)}} \\leftarrow \\  \\phi_i^{(t)} \\ + \\ \\alpha_{i} \\cdot \\delta_t^i \\cdot \\nabla_{\\phi_i}Q_{\\phi_i}(s_t,a_t^i) \\\\\n& \\bm{\\theta_i^{(t+1)}} \\leftarrow \\ \\theta_i^{(t)} + \n\\begin{cases}\n\\alpha_{\\theta} \\cdot \\nabla_{\\theta_i}Q_{\\phi_N}(s_t,\\pi_{\\Theta}(s_t)), \\ \\ Main \\ goal. \\\\ \n\\beta_i \\cdot \\nabla_{\\theta_i}Q_{\\phi_i}(s_t,\\pi_{\\theta_i}(s_t)), \\ \\ \\ Aux \\ goal.\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\\end{small}\n\n\n\n\n\n\\subsection{Model-based Learning Approach with Non-impression Samples}\nTo address SSB problem, we introduce non-impression samples during policy learning. \nTo avoid efficiency problem, we sample a subset from the complete non-impression samples randomly, and about 25\\Moreover, we build a fitting model to assign user feedback for these samples.\nAlgorithm \\ref{alg1} summarizes the details of sampling, simulating and poliy learning approach.\n\n\\begin{algorithm}\n\\small\n\\caption{The training of MMRF.}\n\\label{alg1}\n\\begin{algorithmic}[1]\n\\STATE \\textbf{Initialization}: multi actor-critic agents parameterized by $\\Theta$, $\\Phi$, user feedback simulator $\\mathcal{M}$ parameterized by $\\eta$, replay buffer $\\mathcal{D}$ and simulated buffer $\\mathcal{D}^{\\mathcal{M}}$; \\\\\n\\REPEAT\n    \\FOR{Agent $i = 1$ To $N$}\n        \\STATE Collecting user trajectory $\\tau$ $\\thicksim$ $\\pi_{\\theta_i}$: at round $t$, sample action $a_t^i$ according to $Eq.\\ref{eq10}$ $\\&$ $\\ref{eq2}$, show top-K items $I_k = L_k(s_t, a_t^i)$ with user feedback $r_t^i$, next state $s_{t+1} = s_t \\cup I_k$, store sample $(s_t, a_t^i, r_t^i, s_{t+1})$ in buffer $\\mathcal{D}_i$;\n        \\FOR{$k = 1$ To $m$}\n            \\STATE execute random action $\\widetilde{a}_t^{\\ i}$ to get non-impression sample set $\\widetilde{I_k} = L_k(s_t, \\widetilde{a}_t^{\\ i})$, obtain simulated reward $\\widetilde{r}_t^{\\ i}$ via user feedback simulator $\\mathcal{M}$ as $Eq.\\ref{eq6}$, as well as next state $\\widetilde{s_{t+1}} = s_t \\cup \\widetilde{I_k}$, store sample $(s_t, \\widetilde{a}_t^{\\ i}, \\widetilde{r}_t^{\\ i}, \\widetilde{s_{t+1}})$ in buffer $\\mathcal{D}_i^{\\mathcal{M}}$;\n        \\ENDFOR\n    \\ENDFOR\n    \\STATE \\textbf{Optimising WatchTime and auxiliary goals}: Sample mini-batch from buffer $\\mathcal{D}_i$ and $\\mathcal{D}_i^{\\mathcal{M}}$, update agent $i$, i.e., $\\pi_{\\theta_i}$, $Q_{\\phi_i}$ via mini-batch SGD as $Eq.\\ref{eq3}$;\n    \\STATE \\textbf{Updating User Feedback Simulator} \\bm{$\\mathcal{M}$}: Sample mini-batch from $\\mathcal{D}$, update $\\eta$ via mini-batch SGD.\n\\UNTIL{convergence;}\n\\end{algorithmic}  \n\\end{algorithm}\n\nOur user feedback fitting model adopts a recurrent neural network \\cite{UserBehaviorModel2019}, and sequentially modeling  the transition of user state $s_t$. Since user feedback has multiple aspects, we leverage multi-head outputs, and each head is to fit a specific aspect of user feedback via minimising mean squared error:\n\\begin{small}\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{1pt}\nMSE(\\mathcal{M}) = ||r_t^{p_i} - r_t^i||^2\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nwhere $r_t^{p_i} = \\mathcal{M}(s_t,a_t^i,\\eta)$ is the model prediction parameterized with $\\eta$. In this way, the reward of non-impression samples could be simulated. \n\nHowever, since less exposure opportunity may cause higher estimation uncertainty, those samples with high uncertainty should be more explored, so as to \\textbf{reach a higher upper bound for user satisfaction}. \nThen, in order to estimate the uncertainty, inspired by \\cite{rnd2018}, we extend our model to a siamese structure, with two parallel predictors $r_t^{p_i}$ and $r_t^{p_i'}$ share the same optimizing target as mentioned above. We apply dropout layers to one of the predictors and increase the discrepancy in them. In this way, the uncertainty could be estimated as the inconsistency of $P_{t}^i = (r_t^{p_i}, 1- r_t^{p_i})$ and ${P'}_{t}^i = (r_t^{p_i'}, 1-r_t^{p_i'})$. We regularize the uncertainty as KL-divergence $exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))$. Thus, the reward on non-impression samples could be simulated as:\n\n\n\n\\vspace{-1.0em}\n\\begin{small}\n\\begin{equation} \\label{eq6}\n\\setlength{\\abovedisplayskip}{1pt}\n\\widetilde{r}_t^{\\ i} = mean(r_t^{p_i}, r_t^{p_i'}) \\cdot exp(\\lambda \\cdot KL(P_{t}^i||{P'}_{t}^i))\n\\setlength{\\belowdisplayskip}{1pt}\n\\end{equation}\n\\end{small}\nThe training of user feedback simulator and multiple agents are implemented in an  iterative manner as illustrated in Algorithm \\ref{alg1}.\n\n\n\n\n\n\\begin{table*}[tp]\n  \\centering\\scriptsize\n  \\caption{Model comparison on two datasets at seven aspects of user satisfaction. Bold font\n   indicates the highest performance and * denotes the second highest performance among all baseline methods in each dimension}\n  \\vspace{-1.0em}\n  \\label{tab:rq1}\n  \\begin{tabular}{cc||cc||cc||cc||cc||cc||cc||cc}\n    \\toprule\n    \\multirow{10}{*}{\\rotatebox{90}{KuaiRand-1k}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-4)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{LongView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS \\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6555 & 5.127 & 0.6586 & 1.100 & 0.6472 & 6.170 & 0.6733 & 3.084 & 0.6549 & 3.313 & 0.6130 & 2.771 & 0.6128 & 10.74\n \\\\\n    & Wide \\& Deep & 0.6603 & 5.323 & 0.6583 & 1.135 & 0.6493 & 6.175 & 0.6914 & 3.192 & 0.6886 & 3.020 & 0.6240 & 2.785 & 0.6138 & 10.71\n \\\\\n    & DeepFM & 0.6624 & $5.368^*$ & 0.6317 & 1.086 & 0.6400 & 6.022 & 0.6807 & 3.191 & 0.6697 & 3.133 & 0.6533 & 2.933 & 0.6537 & 10.85\n \\\\\n    & Pareto & 0.6590 & 5.227 & 0.6905 & 1.166 & $0.7056^*$ & $6.318^*$ & 0.6922 & 3.275 & $\\textbf{0.7534}$ & $\\textbf{2.485}$ & 0.6692 & 3.004 & 0.5947 & 9.79\n \\\\\n    & TSCAC & $0.6633^*$ & 5.249 & $\\textbf{0.7002}$ & $\\textbf{1.320}$ & 0.6816 & 6.281 & $0.7253^*$ & $3.519^*$ & 0.7135 & 2.861 & 0.6616 & 2.987 & 0.6650 & 11.04\n \\\\\n    & MASSA & 0.6238 & 4.818 & 0.6927 & 1.272 & 0.6943 & 6.297 & 0.7057 & 3.301 & 0.6126 & 3.528 & $0.6704^*$ & $3.118^*$ & 0.6679 & 11.29\n \\\\\n    & MMRF-CO & 0.6588 & 5.328 & 0.6841 & 1.256 & 0.6909 & 6.275 & 0.6747 & 3.146 & 0.7301 & 2.811 & $\\textbf{0.6734}$ & $\\textbf{3.235}$ & 0.6407 & 11.03\n \\\\\n    & MMRF-DA & 0.6601 & 5.330 & $0.6975^*$ & 1.308 & 0.6964 & 6.303 & 0.7139 & 3.383 & 0.7104 & 2.873 & 0.6695 & 2.992 & $0.6744^*$ & $11.45^*$\n \\\\\n    & MMRF-NS & 0.6620 & 5.347 & 0.6959 & $1.312^*$ & 0.6992 & 6.311 & 0.7139 & 3.383 & 0.7302 & 2.736 & 0.6590 & 2.973 & 0.6711 & 11.35\n \\\\\n    & MMRF & $\\textbf{0.6668}$ & $\\textbf{5.370}$ & 0.6837 & 1.170 & $\\textbf{0.7069}$ & $\\textbf{6.344}$ & $\\textbf{0.7274}$ & $\\textbf{3.597}$ & $0.7316^*$ & $2.795^*$ & 0.6587 & 2.972 & $\\textbf{0.6833}$ & $\\textbf{11.54}$\n \\\\\n    \\midrule\n    \\multirow{10}{*}{\\rotatebox{90}{Real Production Data}} & \\multirow{2}{*}{Methods} & \\multicolumn{2}{c||}{Click$\\uparrow$(1e-1)} & \\multicolumn{2}{c||}{Like$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Follow$\\uparrow$(1e-3)} & \\multicolumn{2}{c||}{Comment$\\uparrow$(1e-2)} & \\multicolumn{2}{c||}{Hate$\\downarrow$(1e-4)} & \\multicolumn{2}{c||}{longView$\\uparrow$(1e-1)} & \\multicolumn{2}{c}{WatchTime$\\uparrow$}\\\\\n    & & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS & GAUC & NCIS\\\\\n    \\specialrule{0em}{1pt}{1pt}\n    \\cline{2-16}\n    \\specialrule{0em}{1pt}{1pt}\n    & BC & 0.6587 & 5.660 & 0.6570 & 2.947 & 0.5981 & 1.933 & 0.6206 & 1.095 & 0.6570 & 3.500 & 0.7279 & 2.875 & 0.6378 & 25.18\n \\\\\n    & Wide \\& Deep & 0.6568 & 5.743 & 0.6544 & 3.003 & 0.5961 & 1.951 & 0.6190 & 1.066 & 0.6551 & 3.439 & 0.7296 & 2.903 & 0.6413 & 25.93\n \\\\\n    & DeepFM & 0.6862 & 5.923 & 0.6484 & 2.987 & 0.6045 & 1.965 & 0.6215 & 1.182 & 0.6868 & 3.269 & $0.7598^*$ & $3.063^*$ & 0.6501 & 26.01\n \\\\\n    & Pareto & 0.6910 & 6.035 & 0.6508 & 2.913 & 0.6241 & 1.995 & 0.6548 & 1.156 & 0.6932 & 2.951 & 0.7422 & 3.033 & 0.6476 & 25.73\n \\\\\n    & TSCAC & 0.7008 & 6.111 & 0.6459 & 2.829 & 0.6198 & 1.931 & 0.6557 & 1.189 & 0.7037 & 2.914 & 0.7374 & 2.958 & 0.6510 & 26.18\n \\\\\n    & MASSA & 0.6885 & 5.845 & $\\textbf{0.6685}$ & $\\textbf{3.197}$ & 0.6309 & 2.1230 & $0.6573^*$ & $1.227^*$ & 0.6918 & 3.110 & 0.7399 & 2.954 & 0.6466 & 24.26\n \\\\\n    & MMRF-CO & $0.7139^*$ & $6.240^*$ & 0.6240 & 2.505 & $0.6367^*$ & $2.134^*$ & 0.6417 & 1.196 & 0.7110 & 2.939 & 0.7580 & 3.038 & 0.6490 & 24.78\n \\\\\n    & MMRF-DA & 0.6957 & 6.079 & 0.6626 & 3.025 & 0.5971 & 1.913 & 0.6308 & 1.111 & 0.6968 & 2.934 & $\\textbf{0.7721}$ & $\\textbf{3.255}$ & $\\textbf{0.6595}$ & $26.85^*$\n \\\\\n    & MMRF-NS & 0.7018 & 6.120 & $0.6634^*$ & $3.030^*$ & 0.6078 & 1.976 & 0.6232 & 1.191 & $0.7094^*$ & $2.903^*$ & 0.7538 & 3.001 & 0.6579 & 26.61\n \\\\\n    & MMRF & $\\textbf{0.7192}$ & $\\textbf{6.270}$ & 0.6421 & 2.889 & $\\textbf{0.6776}$ & $\\textbf{2.389}$ & $\\textbf{0.6719}$ & $\\textbf{1.287}$ & $\\textbf{0.7192}$ & $\\textbf{2.875}$ & 0.7564 & 2.989 & $0.6585^*$ & $\\textbf{26.89}$\n \\\\\n    \\bottomrule\n  \\end{tabular}\n\\vspace{-1em}\n\\end{table*}\n\n\n\n\\section{Experiments}\n\\subsection{Offline Experiments}\n\\subsubsection{Datasets} \nOffline experiments are conducted in an online simulator of one public dataset and one real production dataset.\n\\textbf{Public Dataset: }\nThe public dataset KuaiRand1K \\cite{gao2022kuairand} is collected from a famous video-sharing mobile app and suitable for the offline evaluation of RL methods. We adopt the data pre-processing approach in \\cite{cai2023two}, which chronologically concatenates logs of the same user to form a trajectory. \n\\textbf{Real Production Dataset: }\nThis dataset are traffic logs collected over a week from our large scale video-sharing platform including 245 billion interactions between 1.8 billion users and 1.0 billion videos. Chronologically concatenated logs of the same session form a trajectory.\n\n\n\\subsubsection{Evaluation} \nWe user the \\textit{Normalised Capped Importance Sampling} (\\textbf{NCIS}) approach to evaluate different policies, which is a standard offline evaluation approach for RL-based methods in recommender systems \\cite{zou2019reinforcement}. We also evaluate our method in terms of \\textbf{GAUC}, which is a commonly used metric in recommendation ranking models \\cite{zhou2018deep}.\n\n\\subsubsection{Baselines} \nWe compare our approach with: 1) \\textbf{Industrial commonly applied approaches}, including supervised behavior-cloning (\\textbf{BC}) policy $\\pi_{\\beta}$, {\\textbf{Wide\\&Deep} \\cite{WideandDeep2016} and \\textbf{DeepFM} \\cite{dfm2017} which are supervised by impression signals, samples are weighted via the weighted sum of user feedback. \n2) \\textbf{Multi-objective\/multi-agents RL approaches}, including \\textbf{Pareto} \\cite{ChenXu2021} as a multi-objective RL algorithm that finds the Pareto optimal solution for recommender systems, \\textbf{TSCAC} \\cite{cai2023two} as a two-stage multi-critic algorithm that maximising main response and satisfy the constraints of other auxiliary responses, and \\textbf{MASSA} \\cite{HEXu2020RL} as a multi-agent algorithm, in which a signal network is adopted to handle messages sent by multiple critics and extract useful information to each critic.\n3) \\textbf{Several alternatives of our framework MMRF}, including \\textbf{MMRF-DA} without learning from non-impression samples, \\textbf{MMRF-NS} in which the adoption of user feedback simulator is replaced by a heuristic method, i.e., treating the non-impression samples with a negative constant reward, and \\textbf{MMRF-CO} with no attentive collaboration mechanism but a simple concatenation.\n\n\n\\subsubsection{Experimental Results} \nTable~\\ref{tab:rq1} presents the comparison between our proposed MMRF and other alternatives. \n\nWe can see that our MMRF algorithm significantly outperforms the others: 1) For the main goal (WatchTime), MMRF achieves the highest performance with 7.3\\\nThe results of ablation study are also presented in Table~\\ref{tab:rq1}.\nWe first investigate the effect of collaboration. Note that MMRF outperforms MMRF-CO for 5 out of 7 satisfaction aspects, which confirms that the collaboration mechanism indeed achieves comprehensive improvement.\nWe also explore the effect brought by utilizing non-impression samples. Comparison between MMRF-DA and MMRF shows that as a data argument method, the imitation of user feedback simulator could benefit the learning on sparse signals, with more than 12.7\\\n\n\n\\subsection{Online A\/B Testing}\nTo demonstrate the effectiveness of our proposed framework, we test its performance as well as other alternatives via A\/B testing in our large scale industrial video-sharing platform. In our platform, when a user arrives, these algorithms are expected to rank the candidates, and the top videos will be recommended to the user.\n\n\\subsubsection{Setup} \nHere we explain our adopted online evaluation metrics and experimental details.\n\\textbf{Online Evaluation Metrics: }\nFor the main goal, we look at the averaged total amount of time user spend on the videos, referred to as WatchTime. \nThe total number of the viewed videos among a session (Depth for short) and the the total number of explicit interactions (Follow and Comment) can also speak of user satisfaction.  \n\\textbf{Experimental Details: }\nWe complement our evaluation with a supervised learning-to-rank (LTR) \\cite{liu2009learning} baseline, and take \\textbf{TSCAC} as the alternative.\nTo test the above threee algorithms, we randomly split users on the platform into three backets. The first bucket runs the baseline LTR model, and the remaining buckets run our proposed method and TSCAC. Models are trained for a month and then fixed to test performance within the following one day.\n\n\\begin{table}[H]\n  \\centering\\small\n  \\caption{Performance comparison of our MMRF and the STOA TSCAC with the LTR baseline in online A\/B testing.}\n  \\vspace{-1em}\n  \\label{ab-test}\n  \\renewcommand\\arraystretch{1.0}\n    \\begin{tabular}{l|c|c|c|c}\n    \\hline\n    Online A\/B Test & WatchTime & Depth & Follow & Comment\\\\\n    \\hline\n    LTR & - & - & - & -\\\\\n    TSCAC & +0.32\\    MMRF & +0.55\\    \\hline\n    \\end{tabular}\n  \\vspace{-1em}\n\\end{table}\n\n\\subsubsection{Results Analysis} \nTable \\ref{ab-test} shows the performance improvement of algorithm comparison with the LTR baseline regarding metrics WatchTime, Depth, Follow and Comment. \nAs we can see, our proposed method contributes higher performance improvement than TSCAC which has already been highly optimized for recommender system. \nNote that 0.5\\Figure \\ref{online-period} plots the online performance improvement of our proposed algorithm over the LTR baseline over a sustained 10 days. Note that MMRF is deployed at the end of Day3.\n\n\\begin{figure}[t]\n\\centering\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{WatchTime.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(a)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Depth.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(b)}}\n      \\centering\n    \\end{minipage}    \\hspace{0.1em}\n    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Follow.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(c)}}\n      \\centering\n    \\end{minipage}    \\begin{minipage}[t]{0.5\\linewidth}\n      \\centering\n      \\centerline{\\includegraphics[width=0.99\\linewidth]{Comment.png}}\n      \\vspace{-0.5em}\n      \\centerline{\\footnotesize{(d)}}\n      \\centering\n    \\end{minipage}  \\vspace{-1.0em}\n  \\caption{Online performance improvement of our proposed MMRF over the LTR baseline of each day.}\n  \\label{online-period}\n\\vspace{-1.5em}\n\\end{figure}\n\n\n\n\n\n\\section{Conclusions}\nWe propose MMRF to optimize the cumulative WatchTime and multi-aspect explicit interactions over a session in short video recommendation.\nMMRF maximizes the cumulative WatchTime over a session efficiently and effectively by introducing a collaborative multi-agent environment.\nMoreover, MMRF addresses Sample Selection Bias through introducing non-impression samples and building a feedback fitting model to simulate user feedback for non-impression samples.\nExtensive offline experiments and online A\/B Testing are conducted and the results demonstrate the effectiveness of our proposed method.\nIn fact, MMRF has been deployed in our real-world large-scale video-sharing platform and successfully serving over hundreds of millions of users. \n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}\n\\balance\n\\bibliography{main}\n\n\n\\end{document}\n\n\\endinput\n","rating_answer":"1. Originality: 9\n2. Method: 9\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 9"}
