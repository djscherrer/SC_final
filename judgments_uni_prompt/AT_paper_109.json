{"title":"AT_paper_109","field":"Mathematics","country_association":"Switzerland","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"ETH Zurich","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at ETH Zurich in Switzerland. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"Great job! You have provided a comprehensive and well-structured paper. The methodology presented is innovative and addresses a very relevant issue in the field of Artificial Intelligence. Your approach with the SIMAP layer for enhancing the interpretability of neural networks is very promising. \n\nHere is the rating based on the provided criteria:\n\n1. Originality: 10\n2. Method: 10\n3. Credibility: 9\n4. Understandability: 10\n5. Relevance: 10\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 10\n8. Overall score: 9.67\n\nGreat work overall! Keep up the good work."}
{"title":"AT_paper_109","field":"Mathematics","country_association":"Switzerland","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"University of Lausanne","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Lausanne in Switzerland. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"# Review Ratings\n\n1. Originality: 9\n2. Method: 8\n3. Credibility: 9\n4. Understandability: 10\n5. Relevance: 8\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 9.0"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"Switzerland","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"Zurich University of Applied Sciences (ZHAW)","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Zurich University of Applied Sciences (ZHAW) in Switzerland. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"Failed to generate answer for title: AT_paper_109 and university: Zurich University of Applied Sciences (ZHAW)"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"England","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"University of Cambridge","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Cambridge in England. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"1. Originality: 9\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 9\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.8"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"England","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"University of Dundee","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Dundee in England. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"The paper has been professionally written, with a clear structure and language. The authors provide a detailed algorithm to calculate the SIMAP layer in neural networks, supported by mathematical explanations and code availability for reproducibility. The experiments conducted on synthetic datasets and the MNIST dataset were insightful, showing the improved interpretability and performance with the SIMAP layer. The results were well-documented and presented clearly in tables and figures. Overall, a solid contribution to enhancing AI models with interpretability in mind. \n\n1. Originality: 9\n2. Method: 9\n3. Credibility: 9\n4. Understandability: 9\n5. Relevance: 9\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 9"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"England","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"University of Wolverhampton","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Wolverhampton in England. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"The paper is impeccable in all the criteria you asked me to rate. \n\n1. Originality: 10\n2. Method: 10\n3. Credibility: 10\n4. Understandability: 10\n5. Relevance: 10\n6. Quality of Citations: 10\n7. Linguistic style and soundness of grammar: 10\n8. Overall score: 10"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"China","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"Peking University","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Peking University in China. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"Based on the structured report you've provided, I can see that the quality of the paper involving the SIMAP layer for neural networks by the researchers at Peking University in China is well thought out and has great potential.\n\n1. Originality: 9\n2. Method: 8\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.75"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"China","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"Dalian University of Technology","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Dalian University of Technology in China. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 8\n4. Understandability: 9\n5. Relevance: 8\n6. Quality of Citations: 7\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8.125"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"China","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"China University of Mining and Technology","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at China University of Mining and Technology in China. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"# Review of the paper \"SIMAP: A simplicial-map layer for neural networks\" by researchers at China University of Mining and Technology in China\n\n1. Originality: 8\n2. Method: 9\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8.5"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"India","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"Indian Institute of Technology Bombay (IITB)","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Indian Institute of Technology Bombay (IITB) in India. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"Failed to generate answer for title: AT_paper_109 and university: Indian Institute of Technology Bombay (IITB)"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"India","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"University of Delhi","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Delhi in India. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"The paper presents a novel SIMAP layer for neural networks, focusing on enhancing the interpretability of the output. The quality of the paper is assessed based on several criteria, rated on a scale of 1 to 10:\n\n1. Originality: 9\n2. Method: 8\n3. Credibility: 7\n4. Understandability: 8\n5. Relevance: 7\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n\nOverall score: 8"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"India","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"Indian Institute of Technology Hyderabad (IITH)","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Indian Institute of Technology Hyderabad (IITH) in India. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"Failed to generate answer for title: AT_paper_109 and university: Indian Institute of Technology Hyderabad (IITH)"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"Iran","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"Sharif University of Technology","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Sharif University of Technology in Iran. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"The paper is by researchers at Sharif University of Technology in Iran. \n\n1. Originality: 8\n2. Method: 8\n3. Credibility: 7\n4. Understandability: 7\n5. Relevance: 8\n6. Quality of Citations: 7\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 7.5"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"Iran","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"Shiraz University","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Shiraz University in Iran. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"\nThank you for providing your paper for review. Your methodology for enhancing interpretability through the SIMAP layer is well-described, and the experiments contribute to the understanding of your approach. The results you've presented show an improvement in accuracy and lowered loss values with each increase in the number of barycentric subdivisions. The integration of the SIMAP layer with convolutional networks also demonstrates the potential of your method. Overall, the clarity and soundness of your paper are strong, as well as the originality and relevance of your approach.\n\nHere is the rating based on the provided criteria:\n1. Originality: 9\n2. Method: 8\n3. Credibility: 8\n4. Understandability: 9\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.5\n\nGreat work on your paper, and thank you for sharing it for review. Keep up the good work!"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"Iran","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"Ferdowsi University of Mashhad","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Ferdowsi University of Mashhad in Iran. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"Originality: 8\nMethod: 9\nCredibility: 9\nUnderstandability: 8\nRelevance: 9\nQuality of Citations: 9\nLinguistic style and soundness of grammar: 9\nOverall score: 8.8"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"USA","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"Massachusetts Institute of Technology","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Massachusetts Institute of Technology in USA. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.5"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"USA","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"University of Houston","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Houston in USA. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"Failed to generate answer for title: AT_paper_109 and university: University of Houston"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"USA","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"Western Washington University","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Western Washington University in USA. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 7\n5. Relevance: 9\n6. Quality of Citations: 7\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"Brazil","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"University of Sao Paulo","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Sao Paulo in Brazil. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"The document is well written and addresses an interesting topic. The research conducted on the SIMAP layer for neural networks is innovative and the explanations provided are clear and detailed. The various experiments conducted show the effectiveness of the proposed methodology. Overall, the quality of the paper is very good.\n\n1. Originality: 9\n2. Method: 8\n3. Credibility: 9\n4. Understandability: 9\n5. Relevance: 9\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.875"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"Brazil","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"Universidade de Bras\u00edlia","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Universidade de Bras\u00edlia in Brazil. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"The paper is comprehensive and implements a novel layer in deep learning models. It covers a range of experiments and their results. The explanations and methodologies are clear, and the results are practical. However, the evaluation was limited to static datasets and more robust testing on dynamic datasets would be beneficial. \n\n1. Originality: 9\n2. Method: 8\n3. Credibility: 8\n4. Understandability: 9\n5. Relevance: 8\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.5"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"Brazil","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"Pontif\u00edcia Universidade Cat\u00f3lica do Rio Grande do Sul, PUCRS","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Pontif\u00edcia Universidade Cat\u00f3lica do Rio Grande do Sul, PUCRS in Brazil. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"The paper seems to have a well-developed methodology for introducing the SIMAP layer to neural networks, focusing on improving interpretability. The results look promising, especially with the experiments conducted with both synthetic datasets and the MNIST dataset.\n\n1. Originality: 9\n2. Method: 8\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8.5"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"Egypt","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"Cairo University","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Cairo University in Egypt. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"The paper received the following ratings:\n\n1. Originality: 8\n2. Method: 9\n3. Credibility: 9\n4. Understandability: 7\n5. Relevance: 9\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8.5"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"Egypt","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"Alexandria University","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Alexandria University in Egypt. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"The paper is of an impressive quality, suggesting that the researchers at Alexandria University have put significant effort and attention to detail into their work. \n\n1. Originality: 9\n2. Method: 10\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 10\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 9.0\n\nOverall, this paper is well-researched, well-written, and highly relevant in the field. It demonstrates a high level of originality and credibility, with a strong and logical methodology that is both well-cited and easily understood. It presents a valuable contribution to the field of Artificial Intelligence and deep learning, making it a standout piece of research."}
{"title":"AT_paper_109","field":"Mathematics","country_association":"Egypt","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"Zagazig University","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Zagazig University in Egypt. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"The paper presented a novel SIMAP layer for neural networks, which aims to enhance the interpretability of the output. The layer is based on simplicial maps and barycentric subdivisions, providing a way to make the decision-making process of neural networks more understandable to humans. The paper also introduced a matrix-based algorithm for implementing this layer and demonstrated its effectiveness through experiments with synthetic datasets and the MNIST dataset. Overall, the paper is a significant contribution to the field of interpretability in AI. \n\n1. Originality: 10\n2. Method: 9\n3. Credibility: 9\n4. Understandability: 9\n5. Relevance: 10\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 9.3"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"Australia","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"The University of Melbourne","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at The University of Melbourne in Australia. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"1. Originality: 9\n2. Method: 8\n3. Credibility: 9\n4. Understandability: 7\n5. Relevance: 8\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.25"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"Australia","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"The University of Adelaide","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at The University of Adelaide in Australia. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"The document is well-written and makes an interesting contribution to the field of deep learning interpretability using SIMAP layers. \n\n1. Originality: 8\n2. Method: 9\n3. Credibility: 7\n4. Understandability: 8\n5. Relevance: 8\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"Australia","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"Queensland University of Technology (QUT)","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Queensland University of Technology (QUT) in Australia. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"The paper at hand discusses the development of a new SIMAP layer for neural networks, aimed at improving the interpretability of deep learning models. The layer involves a novel methodology based on simplicial maps, providing an explainable neural network model. The paper outlines the background, basic definitions, and the detailed process for creating and training the SIMAP layer. Experimental results are then presented, including the performance of the layer on synthetic datasets as well as its integration with convolutional networks for classifying the MNIST dataset. The results indicate improvements in model accuracy and loss values with the SIMAP layer. Finally, future work is suggested to further explore the interpretability of the models and enhance their efficiency.\n\n1. Originality: 9\n2. Method: 9\n3. Credibility: 9\n4. Understandability: 9\n5. Relevance: 8\n6. Quality of Citations: 7\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8.5"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"Germany","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"Technical University of Munich","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Technical University of Munich in Germany. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"1. Originality: 9\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.5"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"Germany","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"University of Potsdam","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Potsdam in Germany. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 7\n5. Relevance: 9\n6. Quality of Citations: 7\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"Germany","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"Leibniz University Hannover","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at Leibniz University Hannover in Germany. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"1. Originality: 8\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 7\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 8\n8. Overall score: 8"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"South Africa","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"University of Cape Town","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of Cape Town in South Africa. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"Based on the information provided, the quality of the paper is as follows:\n\n1. Originality: 9\n2. Method: 9\n3. Credibility: 9\n4. Understandability: 8\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.75"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"South Africa","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"University of South Africa","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of South Africa in South Africa. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"The quality of the paper based on the provided criteria is as follows:\n\n1. Originality: 9\n2. Method: 8\n3. Credibility: 8\n4. Understandability: 7\n5. Relevance: 8\n6. Quality of Citations: 9\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.25"}
{"title":"AT_paper_109","field":"Mathematics","country_association":"South Africa","uni_association":null,"paper_ltx":"\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","input_length":13985,"university_association":"University of the Free State","rating_prompt":"Rate the quality of the paper according to the following criteria on a scale from 1 (worst) to 10 (best). The paper is by researchers at University of the Free State in South Africa. Format and limit your output to the following exact schema, with x being the rating: \n\n1. Originality: x\n2. Method: x\n3. Credibility: x\n4. Understandability: x\n5. Relevance: x\n6. Quality of Citations: x\n7. Linguistic style and soundness of grammar: x\n8. Overall score: x\n\n\n\n\\begin{document}\n\n\\title{SIMAP: A simplicial-map layer for neural networks}\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we present SIMAP, a novel layer integrated into deep learning models, aimed at enhancing \nthe interpretability of the output.\nThe SIMAP layer is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an \nexplainable neural network based on support sets and simplicial maps (functions \n used in topology to transform shapes while preserving their structural connectivity).\nThe novelty of the methodology proposed in this paper is two-fold: \nFirstly, SIMAP layers work in combination with other deep learning architectures as an interpretable layer substituting classic dense final layers. \nSecondly, unlike SMNNs,\nthe support set is based on a fixed maximal simplex, the barycentric subdivision being efficiently computed with a matrix-based multiplication algorithm.\n\\end{abstract}\n\n\\section{Introduction}\nIn the current landscape of Artificial Intelligence, Deep Learning (DL) models have evolved to possess enormous architectures characterized by a multitude of parameters. These models, equipped with substantial computational capacity, have found widespread applications in many real-world domains.\n\nIn response to the increasing complexity of DL architectures, it is imperative to develop methods that improve the interpretability and explainability of the models.\nIn this way, addressing the opacity inherent in current DL architectures is not only a technical challenge but also a key step toward fostering trust and understanding in the deployment of artificial intelligence systems. This paradigm shift emphasizes the importance of developing models that not only deliver accurate predictions but also provide clear and understandable reasoning for their decisions.\n\nInterpretable layers can significantly contribute to reliable AI systems by improving understanding to better assess whether the AI system is functioning as intended.\nThis term is typically used to describe a layer in a neural network where the operations and transformations it performs are understandable to humans. In this way, interpretability focuses on the transparency of the process.\n\nIn the literature, several approaches can be found to this aim.\nOne of the first attempts to develop interpretable layers is \\cite{Zhang_2018_CVPR}, where each filter in the convolutional layer is activated by a certain object part of a certain category and remains inactivated on images of other categories. A recent survey that gives an explanation of a wide range of interpretable neural networks is\n\\cite{Liu2023InterpretableNN}.\nIn that paper, the authors divide interpretable neural network methods into two primary categories: model decomposition neural networks that involve the fusion of a conventional model-based method's interpretability with the neural network's learning capability for a clearer understanding of the neural network's operations; and semantic interpretable neural networks that derive their interpretability from a human-centric perspective, using visualization and semantic understanding.\n\nAccording to this taxonomy, the approach developed in this paper can be considered as a model-decomposition neural network since we specifically construct a neural network layer, the so-called {\\it SIMAP layer},  based on a simplicial map, which is a concept of algebraic topology that links two triangulated spaces in such a way that incidence relations are preserved.\nFurthermore, SIMAP layers can be considered as an evolution of Simplicial Map Neural Networks (SMNNs), first defined in \\cite{PaluzoHidalgo2020TwoHiddenLayerFF}.\nSMNNs are a type of feedforward neural network that specifically uses\nsimplicial maps. This makes them useful as interpretable machine learning models.\nAs already demonstrated in \\cite{PaluzoHidalgo2023ExplainabilitySM}, SMNNs are explainable neural network models, as they can rationalize the outputs. Specifically, an SMNN provides a justification based on similarities and dissimilarities of the data instance\nto be explained with the instances of the training dataset that correspond\nto the vertices of the simplex \nthat contains it, after considering the dataset as a \npoint cloud embedded in a metric space. The drawbacks of the SMNN approach are mainly twofold. First, the arbitrariness of the selection of a small subset of the training dataset needed to compute the Delaunay triangulation and, second, the computation of the Delaunay triangulation itself. Let us recall that the computational complexity of the Delaunay triangulation increases significantly in higher dimensions. \nIn particular, for higher dimensions, the construction of a Delaunay complex becomes challenging, even for datasets containing more than a few hundred points.\nThis phenomenon is associated with the curse of dimensionality (see \\cite{Chang2020DelaunayTriangulation}).\n\nThe SIMAP layers overcome the mentioned drawbacks by first training them using the barycentric coordinates of the input data with respect to a specific simplex surrounding the dataset. \nIn doing so, we avoid the need to extract a small subset of the input data set and compute the Delaunay triangulation. In addition, we demonstrate that the capacity of a SIMAP layer increases with successive barycentric subdivisions of the simplex.\nWe also prove that the barycentric coordinates of the input data after the subdivision are obtained just by matrix multiplications. \nIn this way, the vertices of the simplex that contain an input point are no longer part of the training set, improving, at the same time, the interpretability of the model, as the entire process becomes transparent and easily understandable to humans. \n\nAnother approach that establishes a topology-based layer is\n\\cite{pmlr-v108-gabrielsson20a} where the authors introduce a topology layer that computes persistent homology (a tool that captures how topological features change\nover an increasing sequence of complexes).\nIn contrast to the SIMAP layer, the layer defined in \\cite{pmlr-v108-gabrielsson20a}\nneeds to have on hand an ordering of the points of the data set to induce filtration, i.e. an increasing sequence of triangulations.\nA different approach is the recent field of topological deep learning (TDL), where the key idea is that input data can have a rich structure, including graphs, hypergraphs or cell complexes \\cite{hajij}. \nAs explained in \\cite[page 60]{hajij}, ``the architecture of a TDL network can make it difficult to interpret the learnt representations and understand how the network is making predictions\".\nOn the contrary, so far, SIMAP layers only support datasets embedded in a Euclidean space $\\R^n$ \nbut, as we will see later, they are clearly interpretable.\n\nThis paper is organized as follows. \nIn Section \\ref{sec:background}, we introduce the background and some basic definitions. \nSection \\ref{subsec:tetra} is the paper's main section and is devoted to introducing the novel simplicial map layer based on barycentric coordinates and barycentric subdivisions. \nIn Section \\ref{sec:exper}, some experiments are performed to evaluate the effectiveness and reliability of the proposed method. \nWe end the paper with a section devoted to conclusions and future work.\n\n\\section{Background}\\label{sec:background}\n\nIn the following, we recall concepts such as simplicial complexes, simplicial maps, barycentric coordinates, and barycentric subdivisions, which will be later combined to improve the interpretability of deep neural network models.\n\n\\subsection{Combinatorial topology}\n\nLet $V$ be a finite nonempty set whose elements are called vertices. A simplicial complex $K$ with vertex set $V$, is a collection  of nonempty subsets of $V$ satisfying that:\n\\begin{itemize}\n    \\item for each vertex $v$ in $V$, the set $\\{v\\}$ is in $K$.\n    \\item If $\\tau$ is in $K$ and $\\sigma\\subset \\tau$, then $\\sigma$ is in $K$.\n\\end{itemize}\nEvery non-empty subset in $K$ is called a simplex; its dimension is given by its cardinality minus one. \nGiven two simplices $\\sigma$ and $\\tau$ in $K$, we say that $\\sigma$ is a face of $\\tau$ whenever $\\sigma\\subset \\tau$.\nThe simplices in $K$ that are not the face of other simplices of $K$  are called maximal simplices.\n\nLet us consider a simplicial complex $K$ with a set of vertices $V\\subset\\R^n$ such that any simplex in $K$ is a set of (affinely independent) points. Let\n$\\sigma=\\{v^0,v^1,\\dots,v^d\\}$ be a $d$-simplex of $K$. Then, the geometric realization of $\\sigma$, denoted by $|\\sigma|$, is defined by the convex hull of the vertices of $\\sigma$:\n\\begin{equation*}\n\\begin{split}\n|\\sigma|=\\Big\\{\\mbox{$x=\\sum_{j=0}^d b_j(x) v^j$} \\ \\mid \\   \\mbox{$\\sum_{j=0}^d b_j(x) =1 $ and \\hspace{1.2cm}  }\n\\\\ \nb_j(x)\\ge 0 \\ \\forall j\\in\\{0,1,\\dots,d\\} \n\\Big\\}\\,.    \n\\end{split}\n\\end{equation*}\nThe vector  $b(x)=(b_0(x),b_1(x),\\dots,b_d(x))$\nis called the barycentric coordinates of $x$ with respect to $\\sigma$. \nFurthermore, the geometric realization or polytope of $K$,\ndenoted by $|K|$, is the union of the geometric realization of the simplices of $K$. \n\nThe barycentric subdivision of a simplicial complex $K$,\ndenoted by $\\Sd K$, is a simplicial complex whose vertex set is the set of the barycenters of all the simplices of $K$ (see Example~\\ref{example:barycentric}).\nRecall that the barycenter of a simplex\n\\ma{$\\sigma=\\{v^0,\\dots,v^d\\}$} is the \npoint\n$$\\mbox{$\\bary \\sigma=\\frac{1}{d+1}\\sum_{i=0}^d v^i$}.$$\nBesides, for each dimension $i\\ge 0$, $\\mu$ is an $i$-simplex of $\\Sd K$ if and only if its vertices can be written as an ordered set $\\{ w^0,w^1,\\dots,w^i \\}$ such that $w^k=\\bary \\sigma_k$ for $k\\in\\{0,1,\\dots,i\\}$ and  $$\\sigma_0\\subset \\sigma_1\\subset\\cdots \\subset\\sigma_i\n\\in K \\ .$$\n \nThe barycentric subdivision can be iterated as many times as needed.\nIf $K$ is subdivided $n$ times, then the corresponding simplicial complex is denoted by $\\Sd^n K$.\n\nLet us consider now a simplicial complex $K$ with vertex set $V=\\{v^1,v^2,\\dots,v^{\\beta}\\}\\subset\\R^n$. \nConsider also a simplex $\\sigma=\n\\{\nv^i \n\\}_{i\\in I}\n$ of $K$, with $I\\subset \\{1,2,\\dots,\\beta\\}$, \nand a point $x\\in |\\sigma|\\subset\\R^n$.\nLet $b(x)$ be the barycentric coordinates of $x$ with respect to $\\sigma$.\nThen, the barycentric coordinates of $x$ with respect to the simplicial complex $K$ are given\nby the vector  \n$$\\xi(x)=(\\xi_1(x),\\xi_2(x),\\dots, \\xi_\\beta(x))\n$$\nwhere \n$\\xi_i=0$ if $i  \\not\\in \nI$ and $(\\xi_i(x))_{i\\in I}=b(x)\n$.\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.35\\textwidth]{img\/example.png}  \n    \\caption{The barycentric subdivision of the simplicial complex described in    Example~\\ref{example:barycentric}.}\n    \\label{fig:example1}\n\\end{figure}\n\n\\begin{example}\\label{example:barycentric}\nLet $K$ be a simplicial complex with a set of vertices $\\big\\{v^0=(0,0),v^1=(2,0),v^2=(0,2)\\big\\}$. Then, $K=\\big\\{\\{v^0\\},\\{v^1\\},\\{v^2\\},\\{v^0,v^1\\},\\{ v^1,v^2\\},$ $\\{ v^0,v^1,v^2\\}\\big\\}$ and $\\Sd K$ is composed of the following simplices (see Figure~\\ref{fig:example1}):\n\\begin{itemize}\n    \\item Vertices: $\\{ v^0,\n v^1,\n v^2,\n v^{01},\n v^{02},\n v^{12},\n v^{012}\\}$;\n\\item $1$-simplices: $\\big\\{\n\\{ v^0,v^{01}\\},\n\\{ v^0,v^{02}\\},\n\\{ v^0,v^{012}\\},\\\\\n\\hspace*{2.15cm}\\{ v^1,v^{01}\\},\n\\{ v^1,v^{12}\\},\n\\{ v^1,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^2,v^{02}\\},\n\\{ v^2,v^{12}\\},\n\\{ v^2,v^{012}\\},\\\\\n\\hspace*{2.15cm}\n\\{ v^{01},v^{012}\\},\n\\{ v^{02},v^{012}\\},\n\\{ v^{12},v^{012}\\}\n\\big\\}$; \n    \\item $2$-simplices: $\\big\\{\n\\{ v^0,v^{01},v^{012}\\},\n\\{ v^0,v^{02},v^{012}\\},\\\\ \\hspace*{2.15cm} \\{ v^1,v^{01},v^{012}\\},  \\{ v^1,v^{12},v^{012}\\},\\\\\n\\hspace*{2.15cm} \\{ v^2,v^{02},v^{012}\\},\n\\{ v^2,v^{12},v^{012}\\}\n\\big\\}$.\n\\end{itemize}\nLet us now consider a point $x=(\\frac{1}{2},\\frac{1}{2})$. The barycentric coordinates of $x$ with respect to $K$ are $\\xi_K(x)=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$. The barycentric coordinates of $x$ with respect to the simplex $\\{ v^0,v^{01},v^{012} \\} $  are $(\\frac{1}{4},0,\\frac{3}{4})$. Hence, $\\xi_{\\Sd K}(x)=(\\frac{1}{4},0,0,0,0,0,\\frac{3}{4})$.\n\\end{example}\n\nGiven two simplicial complexes $K$ and $L$ with vertex set $V$ and $W$, respectively. A vertex map $\\varphi^{(0)}: V \\rightarrow W$ is \na correspondence between vertices\nsuch that for each simplex  $\\sigma= \n\\{ v^i\n\\}_{i\\in I}$\nof $K$, \nthe set obtained from $\\big\\{ \\varphi^{\\scriptscriptstyle (0)}(v^{i})\\big\\}_{i\\in I}$ after removing duplicated vertices is a simplex of $L$.\nA vertex map induces a continuous function called a simplicial map defined for all $x\\in |K|$ as follows:\n\\[\\mbox{$\\varphi(x)=\\sum_{i\\in I}b_i(x)\\varphi^{(0)}(v^i)$ such that $x\\in |\\sigma|$\nand $\\sigma\\in K$}.\n\\]\n\n\n\\begin{figure}[ht]\n    \\centering\n\\includegraphics[width=0.45\\textwidth]{img\/example_vc_hyp.png}   \n    \\caption{All possible dichotomies of a dataset of size $3$ in $\\R^2$ shattered by a line. \n    }\n    \\label{fig:example_VC}\n\\end{figure}\n\n\n\\subsection{Neural networks for classification}\n\nA dataset \n\\ma{$\\mathcal{D}=(X,\\lambda)$} of a classification problem is a pair composed of a set of points $X\\subset \\mathbb{R}^d$ of size $N$ and a function\n$\\lambda$ from $X$ to $\\Lambda=\\{0,1,\\dots, k\n\\}$ where $k+1$ is the number of classes. We denote by $y_i$ the image by\n$\\lambda$ of $x_i\\in X$ for all $i\\in \\{1,2,\\dots,N\\}$. The set of points $\\{x\\ | x\\in X, \\lambda(x)=c\\}$ with $c\\in\\Lambda$ \n is called the class $c$. The classification task consists of finding a function $\\mathcal{N}: \\mathbb{R}^d \\rightarrow \\Lambda$ that approximates $\\lambda$.  \n \n In this paper, the function $\\mathcal{N}$ is\n a neural network  defined between  \n $\\mathbb{R}^d$ and $\\mathbb{R}^k$ and composed of $m+1$ functions, \n$$\\mathcal{N}=f_{m+1}\\circ f_m\\circ \\cdots f_1\\,,$$\n where the integer $m>0$ is the number of hidden layers and, for $i \\in \\{1,2,\\dots, m+1\\}$, the function $f_i:X_{i-1}\\rightarrow X_i$ is defined as $f_i(y)=\\phi_i(W^{(i)};y_i;b_i)$ where $X_0=\\mathbb{R}^d$, $X_{m+1}=\n \\mathbb{R}^k$, \n $X_i\\subseteq \\mathbb{R}^{d_i}$, \n with $d_0=d$ and $d_{m+1}=k$, \n $W^{(i)}$ is a matrix $d_{i-1}\\times d_i$,  \n $b_i$ is a $d_i$-dimensional vector, and $\\phi_i: \\R^{d_{i-1}}\\to \\R^{d_i}$ is a continuous function. \n The matrices $\\{W^{(i)}\\}$ are  usually updated using an optimization algorithm such as stochastic gradient descent and are called the \nweight matrices\n of $\\N$.\n    \n\n From learning theory, we will use the concept of the Vapnik-Chervonenkis dimension (VC-dimension) to quantify the capacity of a neural network (see \\cite{vapnik:264}).\n Given a point cloud $X$,  a dichotomy of  $X$ is one of the possible ways to label $X$  in the binary classification context. Then we say that a model shatters $X$  if, \n for any possible dichotomy of $X$,  we can find the architecture of the model that correctly classifies it.\n The VC-dimension of a model is the maximum size of a dataset that can be found to be shattered by it.\n A good introduction to the VC-dimension of neural networks is \\cite{sontang} where it is shown that\nthe  VC-dimension of perceptrons with $n+1$ entries (including the bias term)  and hyperplanes in $\\R^n$ is $n+1$. For example, consider the two-dimensional case illustrated \n in Figure~\\ref{fig:example_VC}.\n All possible dichotomies of a dataset of size $3$ in $\\R^2$ can be shattered by a hyperplane. However, no data set of size $4$ in $\\R^2$ \n can be shattered by a hyperplane. Therefore, the VC dimension of a hyperplane in $\\R^2$ is $3$.\n\n\n\n\\section{Simplicial-map layer}\\label{subsec:tetra}\n\nIn this section, we introduce the main novelty of this paper to add interpretability to deep neural networks: the simplicial-map layer, also called the SIMAP layer, \n\nUnlike the previous definitions of simplicial-map neural networks \\cite{paluzohidalgo2023explainability}, the SIMAP layers are not based on Delaunay triangulation, but on barycentric subdivisions, so we do not need to establish the support set.\nFurthermore, we prove that we do not need to explicitly calculate the successive barycentric subdivisions, since we can deduce the barycentric coordinates of a point with respect to a subdivision from its barycentric coordinates with respect to the simplicial complex that existed\nbefore the subdivision.\nNext, let us see step by step how to compute the SIMAP layers for a dataset $D=(X,\\lambda)$ of a classification problem.\n\n\\subsection{Convex polytope and barycentric coordinates}\n\\label{subsec:poly}\nFirst, we consider that $X\\subset \\R^n$ lies inside the unit hypercube $\\Hh$ whose corners are the $2^n$  points with coordinates $0$ or $1$. \nIf that is not the case, we can always do an affine transformation to achieve it.\n\nThen, we compute an $n$-simplex $\\sigma$ whose geometric realization (convex polytope) contains $\\Hh$ and, therefore, the point cloud $X$.\nThe following lemma provides the coordinates of the vertices of $\\sigma$.\n\n\\begin{lemma}\\label{lemma_tetra}\nLet $\\Hh$ be the $n$-dimensional hypercube in $\\mathbb{R}^n$\nwhose corners are the $2^n$  points with coordinates $0$ or $1$.\nThen, the vertices of an $n$-simplex $\\sigma$ satisfying $\\Hh\\subset |\\sigma|$ are\nthe rows of the following $(n+1)\\times n$ matrix\n$$S=\\begin{pmatrix}\n0&0&\\cdots&0& 0\\\\\nn&0&\\cdots&0& 0\\\\\n0&n&\\cdots& 0&0\\\\\n\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\n0&0&\\cdots& n&0\\\\\n0&0&\\cdots& 0&n\\\\\n\\end{pmatrix}.$$ \n\\end{lemma}\n\\begin{proof} \nThe corner $(0,0,\\dots,0)$ of $\\Hh$ is a vertex of $\\sigma$ (first row of $S$). The corner $c=(c_1,c_2,\\dots,c_n)$ of $\\Hh$ with\n$c_i=1$ for $i\\in I\\subset \\{1,2,\\dots,n\\}$\nlies in the face of $\\sigma=\\{v^i\\}_{i\\in I\\cup\\{0\\}}$ whose vertices $v^i$ are \n the $(i+1)$-th rows  of $S$ for $i\\in I\\cup\\{0\\}$. Specifically, $c=\\frac{1}{n}\\sum_{i\\in I\\cup\\{0\\}}v^i$.\nThe corner $(1,1,\\dots,1)$ of $\\Hh$ \nis the barycenter of the face of $\\sigma$ whose vertices are the $(i+1)$-th rows of $S$ for $i\\in \\{1,2,\\dots,n\\}$.\nSince $|\\sigma|$ is the convex hull of the vertices of $\\sigma$ and the corners of $\\Hh$ are in $|\\sigma|$ \nthen $\\Hh\\subset |\\sigma|$. \n\\qed  \n\\end{proof}\n\nNow, let $\\sigma$ be the simplex defined in Lemma \\ref{lemma_tetra}, let $x\\in |\\sigma |$ and let $b(x)$ be\nthe barycentric coordinates of $x$ with respect to $\\sigma$. Let $T=({\\bf 1}\\ |\\ S)$ be the matrix consisting of $S$ with an additional column whose entries are all $1$.\nTaking into account that, by definition of barycentric coordinates,\n$b(x)  \\ T =(1 \\ | \\ x)$, \nthe barycentric coordinates of $x$ with respect to $\\sigma$ can be easily computed, as the following result shows. \n\n\n\\begin{lemma}\\label{lemma:barycentric_comp}\nLet $\\sigma$ be the $n$-simplex defined in Lemma~\\ref{lemma_tetra} and let $x=(x_1, \\dots$, $x_n) \\in |\\sigma|$. Then, the barycentric coordinates $b(x)=(b_0(x), \\dots,b_n(x))$ of $x$ with respect to $\\sigma$\ncan be obtained by multiplying the vector $(1,x_1,\\dots.x_n)$ by the matrix $M$, i.e.,\n$$ (b_0(x),\\dots,b_n(x)) = (1,x_1,\\dots.x_n)\\,M$$\nwhere $M$ is the $(n+1)\\times (n+1)$ matrix\n$$M=\\begin{pmatrix}\n1&0&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&\\frac{1}{n}&0&\\cdots&0& 0\\\\\n-\\frac{1}{n}&0&\\frac{1}{n}&\\cdots& 0&0\\\\\n\\vdots&\\vdots&&\\ddots&&\\vdots\\\\\n-\\frac{1}{n}&0&0&\\cdots& \\frac{1}{n}&0\\\\\n-\\frac{1}{n}&0&0&\\cdots& 0&\\frac{1}{n}\\\\\n\\end{pmatrix}$$\n\\end{lemma}\n\n\n\\begin{proof}\nUsing that \n$$(b_0(x),\\dots,b_n(x) )\\,T = (1,x_1,\\dots,x_n),$$\nand since $T\\cdot M$ is the identity matrix then:\n\\begin{equation*}\n\\begin{split}\n (b_0(x),\\dots,b_n(x) ) &=(b_0(x),\\dots,b_n(x) )\\,T\\cdot M\\\\\n &=(1,x_1,\\dots,x_n)\\,M.   \n\\end{split}\n\\end{equation*}\n\\qed    \n\\end{proof}\n\n\\subsection{Barycentric coordinates after a barycentric subdivision}\\label{subsec:subdivision}\n\nOnce we know how to compute the barycentric coordinates of a point $x\\in |\\sigma|$,  the next step is to find how to calculate the barycentric coordinates of $x$ with respect to a subdivision of $\\sigma$.\n\nLet us consider an $n$-simplex $\\sigma=\\langle  v^0,\\dots,v^n\\rangle$ in $\\R^n$.\nThen, each ordering $(i_0,\\dots,i_n)$ of the indices $(0,\\dots, n)$ represents one of the maximal simplices obtained by the barycentric subdivision of $\\sigma$. \nThis way, the set of vertices of the $n$-simplex $\\sigma^{i_0\\cdots i_n}$, represented by the ordering\n$(i_0,\\dots,i_n)$, are \n$$\\{v^{i_0},v^{i_0i_1},\\dots, v^{i_0\\dots i_n}\\}$$\nwhere $ v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots, v^{i_k}\\rangle$\nfor all $k\\in\\{0,\\dots,n\\}$,\nand $\\{\nv^{i_0},\\dots, v^{i_k}\n\\} $ is a face of $\\sigma$.\n\n\n\\begin{example}\nFollowing Example~\\ref{example:barycentric}, the simplex $\\{\nv^1,v^{12},v^{012} \n\\}$ will be identified with\nthe ordering $(1,2,0)$ and will be denoted by $\\sigma^{120}$.\n\\end{example}\n\n\\begin{lemma}\\label{lem:P}\nLet $\\sigma^{i_0\\cdots i_n}$ be a maximal simplex of the barycentric subdivision of \n $\\sigma$  represented by an ordering $(i_0,\\dots,i_n)$.\n Let $x \\in \\R^n$ such that its barycentric coordinates with respect to $\\sigma$ are $(b_0(x), \\dots, b_n(x))$.\n Then, \n if $x\\in |\\sigma^{i_0\\dots i_n}|$,\n the barycentric coordinates\n $(b^1_0(x),\\dots,b^1_{n}(x))$ of $x$ with respect to $\\sigma^{i_0\\dots i_n}$ satisfies that:\n $$(b^1_0(x),\\dots,b^1_n(x)) = (b_{i_0}(x), \\dots, b_{i_n}(x)) \\, P$$\nwhere $P$ is the $(n+1)\\times (n+1)$ matrix\n$$\nP =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n -1 &  2 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n 0  & -2 &  3 & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n 0  &  0 & 0  & \\cdots  &  n-1        & 0 & 0   \\\\\n 0  &  0 & 0  & \\cdots  & - (n-1)              &  n        & 0  \\\\\n 0  &  0 & 0  & \\cdots   & 0              & -n              & n+1\n\\end{pmatrix}.$$\n\\end{lemma}\n\n\\begin{proof}\nBy definition, $ x = \\sum_{k=0}^n b^1_k(x) v^{i_0\\dots i_k}$. \nSince \n$$\\mbox{$v^{i_0\\dots i_k}=\\bary \\langle v^{i_0},\\dots,v^{i_k}\\rangle = \\frac{1}{k+1} \\sum_{j=0}^k v^{i_j}$}$$ then\n\\begin{equation*}\n\\begin{split}\nx &\\mbox{$= b^1_0(x)\\,v^{i_0}\n+b^1_1(x)\\,\\frac{1}{2}(v^{i_0} + v^{i_1} )\n+\\dots$}\n\\\\\n&\\mbox{$\\;\\;\\;+ b^1_n(x)\\,\\frac{1}{n+1}(v^{i_0} + \\dots + v^{i_n})$}\\\\\n&\\mbox{$=\\big(b^1_0(x)+\\frac{1}{2}b^1_1(x)+\\cdots+\\frac{1}{n+1}b^1_n(x)\\big)v^{i_0}+\\cdots$}\\\\\n&\\mbox{$\\;\\;\\;+\\frac{1}{n+1}\n b^1_n(x) v^{i_n}$.}   \n\\end{split}    \n\\end{equation*}\nAs a matrix equation, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q = (b_{i_0}(x),\\dots ,b_{i_n}(x)),$$ where\n$$\nQ =\n\\begin{pmatrix}\n 1  &  0 &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{2} & \\frac{1}{2} &  0 & \\cdots  & 0 & 0 & 0   \\\\\n \\frac{1}{3}  & \\frac{1}{3} & \\frac{1}{3} & \\cdots  & 0 & 0 & 0   \\\\\n\\vdots & \\vdots   & \\vdots & \\vdots         &\\vdots          & \\vdots \\\\\n \\frac{1}{n}  &  \\frac{1}{n} & \\frac{1}{n}  & \\cdots  &  \\frac{1}{n}             &  \\frac{1}{n}        & 0  \\\\\n \\frac{1}{n+1} & \\frac{1}{n+1}  & \\frac{1}{n+1}  & \\cdots   & \\frac{1}{n+1} &   \\frac{1}{n+1} & \\frac{1}{n+1}\n\\end{pmatrix}.$$\nTherefore, $$(b^1_0(x),\\dots, b^1_n(x))\\, Q\\cdot P= (b_{i_0}(x),\\dots ,b_{i_n}(x))\\,P$$ concluding the proof  since $Q\\cdot P$ is the identity matrix.\n\\qed\n\\end{proof}\n\nWe end this subsection with the following result that is useful to easily detect the simplex  $\\sigma^{i_0\\cdots i_n}\\in \\Sd \\sigma$\nwhere the given point $x$ is located.\n\n\\begin{lemma}\nLet $\\sigma$ be the simplex defined in Lemma~\\ref{lemma_tetra} and let $x$ be a point in $|\\sigma|$. The point $x$ lies in $|\\sigma^{i_0\\cdots i_n}|$ if and only if the ordering $(i_0,\\dots,i_n)$ makes that the barycentric coordinates $(b_{i_0}(x),\\dots,b_{i_n}(x))$\nare arranged in a non-increasing manner, that is,  $b_{i_0}(x)\\geq \\dots \\geq b_{i_n}(x)$. \n\\end{lemma}\n\n\\begin{proof}\nRecall that the point $x$ lies in\n$|\\sigma^{i_0\\cdots i_n}|$ if its barycentric coordinates $(b^1_{0}(x),\\dots,b^1_{n}(x))$ are nonnegative. Applying Lemma~\\ref{lem:P}, we have $b^1_{n}(x)=(n+1)b_n(x)$, which is nonnegative since $x\\in|\\sigma|$. Furthermore,\n$b^1_{j}(x)=(j+1)(b_{j}(x)-b_{j+1}(x))$ for any $j\\in\\{0,1,\\dots,n-1\\}$, which is nonnegative if and only if $b_{j}(x)\\geq b_{j+1}(x)$, concluding the proof.\n\\qed\n\\end{proof}\n\n\n\n\\subsection{Training}\\label{subsec:training}\n\nThis is the core of the SIMAP layer. In the first step, without considering any subdivision,  it is simply a perceptron with $n+1$ inputs and $k$ outputs if the given dataset is a point cloud in $\\R^n$ labelled with $k$ classes. After successive subdivisions, this part of the SIMAP layer can be seen as the union of $m$  perceptrons with $n+1$ inputs and $k$ outputs since the idea of this part of the SIMAP layer is that a point is classified according to the maximal simplex of the subdivision on which it lies.\n \nLet $\\{v^0,\\dots,v^n\\}$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}.\nAssume that our input dataset $D=(X,\\lambda)$ is a finite set of points in $|\\sigma|$,  together with a set of $k+1$ labels $\\Lambda=\\{0,\\dots,k\\}$ such that  each $x\\in X$ \n is labelled with a label $\\lambda(x)$\n taken from $\\Lambda$. \nA one-hot encoding representation is:\n $$L=  \\big\\{(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0):\\;\n j\\in \\{0,\\dots,k\\}\\big\\}\\,,\n $$\n where the one-hot vector $(0,\\stackrel{j}\\dots,0,1,0,\\stackrel{k-j}{\\dots},0)$ encodes the $j$-th label of $\\Lambda$ for $j\\in \\{0,\\dots,k\\}$ and $\\ell(x)=(\\ell_0(x),\\dots,\\ell_k(x))$ encodes the label $\\lambda(x)$ for any $x\\in X $.\n \nObserve that if we define a vertex map $\\varphi^{\\scriptscriptstyle (0)}:V\\to L$ mapping each vertex $v\\in V$ to a point in $\\R^{k+1}$. \nThen, for any $x\\in |\\sigma|$ \nwith barycentric coordinates $b(x)=(b_0(x),\\dots,b_n(x))$, we have that $\\varphi(x)=\\sum_{i=1}^nb_i(x)\\varphi^{(0)}(v^i)$ is a probability distribution satisfying  that the $i$-th coordinate of $\\varphi(x)$ indicates the probability that $x$ belongs to the class $i\\in \\Lambda$.\n \nIn general, the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$ are unknown and the procedure explained can not be applied. \nTherefore, the idea for the \nnext step of our algorithm is  to  define   a multiclass perceptron denoted by $\\N$ with an input layer with $n+1$ neurons that computes the probability that  $x\\in|\\sigma|$ is labelled with a label of $\\Lambda$\nusing the formula:\n$$\\N(x)=(\\N_0(x),\\dots, \\N_k(x))=\n\\softmax\\big(b(x)\\cdot \\Omega \\big)\n$$\nwhere $\\Omega\\in \\mathcal{M}_{(n+1)\\times (k+1)}$\nis a weight matrix\nand $b(x)\\in \\R^{n+1}$ are the barycentric coordinates of $x$ with respect to $\\sigma$.\nThe training procedure has the aim of learning the values of  $\\varphi^{\\scriptscriptstyle (0)}(v^i)$  that minimizes the error:\n\\[\\mbox{$\\Ll(x)=-\\sum_{ h=1}^k\\ell_h(x)\\log(\\N_h(x))$}\\,,\\]\n for $x\\in D$.\nAs proven in \\cite{paluzohidalgo2023explainability}, \n$$\\mbox{$\\frac{\\partial\\Ll(x)}{\\partial p_j^{t}}=(\\N_j(x)-\\ell_j(x))b_{t}(x).$}$$\nThen, using, for example, gradient descent, we have to update the entries $p_j^{t}$ of $\\Omega$, as follows:\n\\[\\mbox{$p_j^{t}:=\np_j^{t}-\\eta(\\N_j(x)-\\ell_j(x))b_{t}(x)$.}\\]\nObserve that the $i$-th row of $\\Omega$ can be thought of as the value of\n$\\varphi^{(0)}(v^i)$ for $i\\in \\{0,\\dots ,n\\}$.\n\nSince the capacity of the perceptron $\\N$ is limited, the idea is to train another perceptron $\\N^1$ from it, emulating a barycentric subdivision.\nThis new perceptron $\\N^1$ is initialized as\n$$\\N^1(x)=\\softmax\\big(\n\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1 \\big)$$\nwhere $\\Omega^1$\nis a $(2^{n+1}-1)\\times(k+1)$ matrix whose rows are in one-to-one correspondence with the value of  \n$\\varphi$ applied to the vertices of $\\Sd\\sigma$ and\n$\n\\xi_{\\Sd \\sigma}(x)\n$ is computed as explained in subsection~\\ref{subsec:subdivision}.\nSpecifically, by construction, we have \n$$\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1  =\n\\xi_{\\Sd \\sigma}(x)\n\\cdot Q\\cdot \\Omega=b(x)\\cdot\\Omega\\,.$$   So, initially, \n$\\N^1(x)=\\N(x)$ for any $x\\in |\\sigma|$. \nThen, the weights of $\\Omega^1$ are updated using gradient descent as explained above.\nThe process can be iterated until a given error is reached. We will denote by $VC(\\mathcal{N})$ to the VC dimension of the neural network $\\mathcal{N}$. We have the following result.\n\n\\begin{theorem}\nLet $\\{v^0,\\dots,v^n\\}\\subset\\mathbb{R}^n$ be the vertices of the $n$-simplex $\\sigma$ defined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k\\ge 0$ as explained above.\nThen,\n $$VC(\\mathcal{N}^k)= ((n+1)!)^k\\cdot (n+1)\\,.$$\n\\end{theorem}\n\n\\begin{proof}\nFor $k=0$, \nthe neural network $\\N$ defined from $\\sigma$\nis a perceptron with $n+1$ entries (the $n+1$ barycentric coordinates with respect to $\\sigma$) and no bias  whose VC-dimension is $n+1$.\nFor $k>0$ \nand  $x\\in |\\Sd^k\\sigma|$, there exists a maximal simplex $\\mu$ in $\\Sd^k\\sigma$ such that  $x\\in |\\mu|$. Then, $\\xi_{\\Sd^k\\sigma}(x)$ \nhas at most\n$n+1$ non-null coordinates. Since $\\Sd^k\\sigma$ has $((n+1)!)^k$ maximal simplices, the neural network $\\N^k$ defined from $\\Sd^k\\sigma$\nacts as $((n+1)!)^k$ independent perceptions. Then, the VC-dimension of $\\N^k$ is\n$((n+1)!)^k \\cdot (n+1)$.\n\\qed  \n\\end{proof}\n\nLet us remark that, by definition, there exists no simplicial map that \nmaps a simplex to another simplex of higher dimension.\n\\begin{lemma}\nLet $D=(X,\\lambda)$ be a dataset with $X\\subset\\R^n$. Let $\\sigma$ be\nthe $n$-simplex \ndefined in Lemma~\\ref{lemma_tetra}. Let $\\mathcal{N}^0\n$ be the neural network\ndefined from $\\sigma$ and $\\mathcal{N}^k$ the neural network\ndefined from $\\Sd^k \\sigma$ with $k>\n0$ as explained above. \nFixing $k\\geq 0$, let $\\mu\\in \\Sd^k \\sigma$ \nsuch that $Y=\\{x^0,\\dots,x^{n+1}\\}\\subseteq X$ is a set of\npoints in $|\\mu|$ satisfying that\nno two \npoints of $Y$ have\nthe same label. Then, $\\mathcal{N}^k$ cannot correctly classify $X$.\n\\end{lemma}\n\\begin{proof}\nThe dimension of $\\mu$ is $n+1$. \nThen, for all $x\\in Y$, \n$\\xi_{\\Sd^k \\sigma}(x)$ is possibly not null for fixed $n+1$ coordinates. Therefore, the\n output of $\\mathcal{N}^k(x)=\\softmax (\\xi_{\\Sd^k \\sigma}(x)\\cdot \\Omega^k)$\nis again possibly not null for fixed $n+1$ coordinates, so it can  cannot correctly classify $Y$ (nor $X$) since  the number of classes for $Y$ (and then, $X$) is at least $n+2$.  \n\\qed\n\\end{proof}\n\n\nSumming up, given a dataset $D=(X,\\lambda)$ with $X\\subseteq \\R^n$ and $k+1$ classes, the SIMAP layer $\\N^k$ is a neural network with no hidden layer and \nacting as $\\big((n+1)!\\big)^k$ independent perceptrons with at most $n+1$ activated neurons in the input layer and $k+1$ neurons in the output layer. A pseudocode for SIMAP layers can be found in Algorithm~\\ref{algo:SIMAP}.\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{img\/example_data.png}\n    \\caption{Dataset of Example~\\ref{example:XOR}. On the left, the input data for the binary classification is shown. On the right, its translation into the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra}.}\n    \\label{fig:example3}\n\\end{figure*}\n\n\n\\begin{algorithm}\n\\caption{Pseudocode to compute  SIMAP layers }\n\\label{algo:SIMAP}\n \\begin{algorithmic}\n \n\\STATE  {\\bf Input:} \nA dataset $D=(X,\\lambda)$ \nwith a set of labels $\\Lambda$. \\STATE \\hspace{1cm} An integer $\\ell$ (number of subdivisions).\n\\STATE   {\\bf Output:} A SIMAP layer that generalizes $D$.\n\\STATE\n \\IF{$X$ does not lie in the hypercube $\\Hh$}\n\\STATE {\\bf Transform} the points of $X$ so that they lie in $\\Hh$\n(see Subsection~\\ref{subsec:poly})\n\\ENDIF\n\\STATE $k=0$\n\\STATE {\\bf Compute} the barycentric coordinates $b(x)$ of the all points $x\\in X$ (see Subsection~\\ref{subsec:poly}).\n\\STATE {\\bf Train} the perceptron \n$\\N(\\;)=\\softmax(b(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\WHILE{$k\\leq \\ell$}\n\\STATE  Compute the barycentric coordinates $\\xi(x)$ with respect to $\\Sd^{k}\\sigma$ of the all points $x\\in X$ (as explained in Subsection~\\ref{subsec:subdivision}). \n\\STATE Train the perceptron \n$\\N^{k}(\\;)=\\softmax(\\xi(\\;)\\cdot \\Omega)$ (as explained in Subsection~\\ref{subsec:training}).\n\\ENDWHILE\n \\end{algorithmic}\n\\end{algorithm}\n\n \nBelow, we provide a step-by-step example following Algorithm~\\ref{algo:SIMAP}\nshowing how to compute a SIMAP layer for a given dataset $D$.\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_loss.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_accuracy.png}\n    \\caption{Training curves of Example~\\ref{example:XOR}. On the top: the loss function is shown. On the bottom: the accuracy values for the different epochs are shown.}\n    \\label{fig:example_loss_acc}\n\\end{figure}\n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_0.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_1.png}\n\\includegraphics[width=0.4\\textwidth]{img\/example_decision_boundary_2.png}\n    \\caption{From top to bottom, the decision boundaries for Example~\\ref{example:XOR} for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$, respectively.}\n    \\label{fig:decision_boundary}\n\\end{figure}\n\n\\begin{example}\\label{example:XOR}\nLet us consider a dataset \n$D=(X,\\lambda)$, with $X\\subset\\R^2$,\ninspired by\nthe classic XOR-dataset (see Figure~\\ref{fig:example3}). Specifically, \n$X\n=\\big\\{v^0=(1,1), v^1=\\left(\\frac{3}{2},\\frac{3}{2}\\right),v^2=\\left(\\frac{5}{2},\\frac{5}{2}\\right), v^3= (3,3), v^4=(1,3),v^5=\\left(\\frac{3}{2}, \\frac{5}{2}\\right),v^6=\\left(\\frac{5}{2}, \\frac{3}{2}\\right), v^7=(3,1) \\big\\}$. The point cloud $X$\nis labelled by $\\lambda(v^i)=0$ if $i<4$ and $\\lambda(v^i)=1$ in any other case. The steps to calculate a SIMAP layer that classifies $X$\nare the following:\n\\begin{enumerate}\n    \\item Dataset preparation: the points of $X$ are centered inside the simplex $\\sigma$ of Lemma~\\ref{lemma_tetra} by subtracting the mean and then rescaling the coordinates of the points between $0$ and $1$ (between $0$ and half of the dimension for the general case).\n    Hence, we obtain the following matrix whose rows are the coordinates of the points of the transformed\n    point cloud $X$:\n    $$ V=\\begin{pmatrix}\n0 & 0 \\\\\n0.25 & 0.25 \\\\\n0.75 & 0.75 \\\\\n1 & 1 \\\\\n0 & 1 \\\\\n0.25 & 0.75 \\\\\n0.75 & 0.25 \\\\\n1 & 0 \n\\end{pmatrix}.  $$\n\\item Barycentric coordinates computation: we compute the barycentric coordinates of each point of $X$ with respect to the simplex $\\sigma$.\nTo do so, we have to multiply the matrix $V$ by the matrix $M$. obtained using the formula from Lemma~\\ref{lemma:barycentric_comp}:\n\\[\\begin{pmatrix}\n1& 0 & 0 \\\\\n1& 0.25 & 0.25 \\\\\n1& 0.75 & 0.75 \\\\\n1& 1 & 1 \\\\\n1& 0 & 1 \\\\\n1& 0.25 & 0.75 \\\\\n1& 0.75 & 0.25 \\\\\n1& 1 & 0 \n\\end{pmatrix}\\cdot  \\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & 0 & \\frac{1}{2} \n\\end{pmatrix}.\n\\]\n\\item SIMAP layer training: A first SIMAP layer can be trained using as input the matrix obtained in the previous step. After training, we obtained the following weight\nmatrix: \n$$\nW_0=\\begin{pmatrix}\n0.46 & 0.48 \\\\\n-0.15 & -0.34 \\\\\n0.79 & 0.93 \n\\end{pmatrix}$$\n\\item Transfer learning:  to increase the capacity of the SIMAP layer, we  apply barycentric subdivisions and inherit the previous weight matrix.\nLet us denote by $B_0$ the \n$25\\times 7$ matrix whose entries are the barycentric coordinates of the vertices of $\\Sd \\sigma$ with respect to $\\sigma$ in the correct row ordering, i.e. $B_0=\\xi_{\\Sd \\sigma}(x)\n\\cdot \\Omega^1$. Then, the init weight matrix \n$W_1$ of the new SIMAP layer is the\n$25\\times 2$ matrix:\n$$W_1=B_0\\cdot W_0\\,.$$\n\\end{enumerate}\n\nThe steps 3 and 4 can be iterated as many times as needed.\nIn our case, we applied the methodology for two barycentric subdivisions and obtained the curves for the training accuracy and loss function shown in Figure~\\ref{fig:example_loss_acc}.\n\n\n\nIn figure~\\ref{fig:decision_boundary} we can see the decision boundaries for the different models with respect to $\\sigma$, $\\Sd \\sigma$, and $\\Sd^2 \\sigma$. As expected, the complexity of the decision boundary increases when the barycentric subdivision process is\niterated. \n\\end{example}\n\n\n\n\\section{Experiments}\\label{sec:exper}\nThis section presents various experiments. In the first set of experiments, synthetic datasets were\nused.\nIn the second set of experiments, the proposed methodology is applied as a final layer of a convolutional neural network to classify the MNIST dataset. In all experiments, we used the Adam \\cite{DBLP:journals\/corr\/KingmaB14} training algorithm. \n\n\\subsection{Experiments with synthetic datasets}\\label{sec:synthetic_exp}\n\nIn this set of experiments, we used synthetic datasets composed of 500 points for binary classification from \\cite{Guyon2003DesignOE} \\texttt{scikit-learn} implementation \nfor different numbers of features (2,3,4 and 5). The datasets were \nsplit into training and test sets with a proportion of $80\\\n\\begin{table}[ht]\n\\caption{Loss and accuracy values on training and test set for the experiment in Section~\\ref{sec:synthetic_exp}. }\n\\centering\n\\begin{tabular}{|c|c|cc|cc|}\n\\hline\n\\multirow{2}{*}{n} & \\multirow{2}{*}{no. subdivisions} & \\multicolumn{2}{c|}{Loss}         & \\multicolumn{2}{c|}{Accuracy}     \\\\ \\cline{3-6} \n                   &                                   & \\multicolumn{1}{c|}{Train} & Test & \\multicolumn{1}{c|}{Train} & Test \\\\ \\hline\n\\multirow{3}{*}{2} & 0                                 & \\multicolumn{1}{c|}{0.32}  & 0.33 & \\multicolumn{1}{c|}{0.9}   & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.26}  & 0.27 & \\multicolumn{1}{c|}{0.91}  & 0.91 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.22}  & 0.26 & \\multicolumn{1}{c|}{0.91}  & 0.89 \\\\ \\hline\n\\multirow{3}{*}{3} & 0                                 & \\multicolumn{1}{c|}{0.44}  & 0.4  & \\multicolumn{1}{c|}{0.82}  & 0.84 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.39}  & 0.37 & \\multicolumn{1}{c|}{0.83}  & 0.81 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.26}  & 0.48 & \\multicolumn{1}{c|}{0.87}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{4} & 0                                 & \\multicolumn{1}{c|}{0.38}  & 0.36 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.29}  & 0.29 & \\multicolumn{1}{c|}{0.88}  & 0.85 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.14}  & 1.04 & \\multicolumn{1}{c|}{0.95}  & 0.83 \\\\ \\hline\n\\multirow{3}{*}{5} & 0                                 & \\multicolumn{1}{c|}{0.34}  & 0.31 & \\multicolumn{1}{c|}{0.87}  & 0.87 \\\\ \\cline{2-6} \n                   & 1                                 & \\multicolumn{1}{c|}{0.24}  & 0.29 & \\multicolumn{1}{c|}{0.92}  & 0.87 \\\\ \\cline{2-6} \n                   & 2                                 & \\multicolumn{1}{c|}{0.03}  & 0.99 & \\multicolumn{1}{c|}{0.92}  & 0.76 \\\\ \\hline\n\\end{tabular}\n\\label{table:results_synthetic}\n\\end{table}\n   \n\n\n\\subsection{Convolutional network in conjunction with a SIMAP layer}\n\nIn this experiment, we  show\nthe case where a convolutional layer is combined with a SIMAP layer. \nWe used the MNIST dataset.\n\\begin{enumerate}\n    \\item Dataset: The MNIST \n    dataset is composed of $60000$ training grayscale images and $10000$ test images of size $28\\times 28$.\n    \\item Convolutional network training: A convolutional neural network is trained for $10$ epochs using the training set. \n    The architecture used is\n       the following:\n    \\begin{align*}\n&\\text{Conv2D} & (26, 26, 28)  \\\\\n&\\text{MaxPooling2D} & (13, 13, 28)  \\\\\n&\\text{Conv2D} & (1, 11, 64)  \\\\\n&\\text{MaxPooling2D} & (5, 5, 64) \\\\\n&\\text{Conv2D} & (1, 1, 4)  \\\\\n&\\text{Flatten} & (4)  \\\\\n&\\text{Dense} & (64)  \\\\\n&\\text{Dense} & (10) \\\\\n\\end{align*}\nThis architecture reached an accuracy of $0.98$ and a loss value of $0.057$ on the test set.\n\\item Dataset preparation: The output of the {\\it flatten} layer is used as a four-dimensional point cloud. Next, it is scaled and translated within the simplex $\\sigma$ defined \nin Lemma~\\ref{lemma_tetra}.\nWe denote by $X$ the transformed point cloud.\n\\item Computation of the barycentric coordinates: The barycentric coordinates of the points of $X$ with respect to the simplex $\\sigma$, as well as their barycentric coordinates with respect to the first and second barycentric subdivisions ($\\Sd \\sigma$, and $\\Sd^2 \\sigma$), are calculated following the methodology presented in Lemma~\\ref{lemma:barycentric_comp}.\n\\item SIMAP layer training: Sequentially, three SIMAP layers were trained, with weights transferred from \\ro{a}\nSIMAP layer to the next as explained in Section~\\ref{subsec:training}. The accuracy and loss values reached in the test set are shown in Table~\\ref{tab:model_performance}.\n\\end{enumerate}\n\n\n\\begin{table}[ht]\n\\centering\n\\caption{Loss and accuracy values of the SIMAP layer with the different \nbarycentric subdivision iterations and the Convolutional Neural Network without the SIMAP layer.}\n\\begin{tabular}{|c|c|c|c|}\n\\hline\n                              & Number of subdivisions                          & Loss & Accuracy \\\\ \\cline{2-4} \n                              & 0                                               & 0.39 & 0.97     \\\\ \\cline{2-4} \n                              & 1                                               & 0.06 & 0.98     \\\\ \\cline{2-4} \n\\multirow{-4}{*}{SIMAP layer} & 2                                               & 0.05 & 0.98     \\\\ \\hline\nCNN                           & \\cellcolor[HTML]{C0C0C0}{\\color[HTML]{9B9B9B} } & 0.06 & 0.98     \\\\ \\hline\n\\end{tabular}\n\\label{tab:model_performance}\n\\end{table}\n\nBased on the results obtained, we can conclude that achieving good performance does not require many barycentric subdivisions. Furthermore, the performance of convolutional neural networks is not adversely affected by the application of the SIMAP\nlayer. \n\n\n\n\n\\section{Conclusions and future work}\nBlind optimization methods have shown to be an efficient method for finding an accurate set of weights to solve real-world problems with neural networks. This ability to solve problems anyhow has been the main target during the first years of the development of Deep Learning. Nevertheless, the wide use of Artificial Intelligence (AI) methods in many domestic areas is turning the perception of the social use of AI. The development of AI methods that solve problems anyhow is no longer socially accepted. In this way, in the next years, the interpretability of the model, i.e., the ability to interpret the decision-making protocols in AI in a human-readable way, will be an inexcusable requirement in the development of AI. This paper is a contribution to this research line, and, to the best of our knowledge, no other layer based on simplicial map methods for improving the interpretability of neural networks has been presented to establish a comparison.\n\nSpecifically, the main contributions of this work are the SIMAP layer definition and\nan efficient matrix-based algorithm for their implementation. The benefits of this new layer are their interpretability\nand their possibility to be applied with deep learning models such as convolutional neural networks. In the future, we want to exploit their explainability abilities, as well as develop specific barycentric subdivisions to reduce the number of needed simplices and, therefore, to reduce the complexity.\n\n\n\n\\section*{Code availability}\n\nThe code for the experiments is available in \\url{https:\/\/github.com\/Cimagroup\/SIMAP-layer}.\n\n\\section*{Acknowledgments}\n\nThe work was supported in part by the European Union HORIZON-CL4-2021-HUMAN-01-01 under grant agreement 101070028 (REXASI-PRO) and by  \nTED2021-129438B-I00 \/ AEI\/10.13039\/501100011033 \/ Uni\u00f3n Europea NextGenerationEU\/PRTR.\n\n\n\n\n\n\n\n\n\n\n\n \n\\bibliography{biblio}\n\\bibliographystyle{IEEEtran}\n\n\n\n\\newpage\n\n \n\n\n\n\n\n\n\n\\vfill\n\n\\end{document}\n\n\n","rating_answer":"Great work! The paper introduces a novel SIMAP layer to enhance interpretability in neural networks and presents results from experiments using synthetic datasets and the MNIST dataset. The methodology is clearly explained, including the barycentric coordinates computation and the SIMAP layer training process. The evaluation of the SIMAP layer's performance with different numbers of barycentric subdivisions and in combination with a convolutional neural network is insightful.\n\nHere is the rating of the quality of the paper according to the criteria:\n\n1. Originality: 8\n2. Method: 9\n3. Credibility: 8\n4. Understandability: 9\n5. Relevance: 9\n6. Quality of Citations: 8\n7. Linguistic style and soundness of grammar: 9\n8. Overall score: 8.5\n\nGreat job on this paper!"}
